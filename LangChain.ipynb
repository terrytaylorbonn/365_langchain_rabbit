{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ae9b5dd07f04243938a057bb5ca02a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c53774b5d09d4ae480def47682b65adb",
              "IPY_MODEL_b6ae4ad6a11147e081a14480c638ed9b",
              "IPY_MODEL_5aa838a2059f455fab7e50341934bcb3"
            ],
            "layout": "IPY_MODEL_8e9238bc42734ad9a6e5c831a54bdcd6"
          }
        },
        "c53774b5d09d4ae480def47682b65adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6792a2717af4c0b9644f633af2c53b5",
            "placeholder": "​",
            "style": "IPY_MODEL_76bdf2be51b144e3960b7c2e82d20940",
            "value": "modules.json: 100%"
          }
        },
        "b6ae4ad6a11147e081a14480c638ed9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca708d891012453ea7dd04cc0cb1fab8",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07acb5912ef04ebda8dfb3c282d9011b",
            "value": 349
          }
        },
        "5aa838a2059f455fab7e50341934bcb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8a303daef8a4369a9fd18424dadf7b7",
            "placeholder": "​",
            "style": "IPY_MODEL_e1e31a3507034159b90823c16940164c",
            "value": " 349/349 [00:00&lt;00:00, 23.9kB/s]"
          }
        },
        "8e9238bc42734ad9a6e5c831a54bdcd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6792a2717af4c0b9644f633af2c53b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76bdf2be51b144e3960b7c2e82d20940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca708d891012453ea7dd04cc0cb1fab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07acb5912ef04ebda8dfb3c282d9011b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8a303daef8a4369a9fd18424dadf7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e31a3507034159b90823c16940164c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aced000579ac473988356c0228a85951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c69eedaad2354169aaf226b24101a030",
              "IPY_MODEL_c5c61252502941aaa12389b8c9c49fb4",
              "IPY_MODEL_3b627e50ef294b2d9d7c012e17366a31"
            ],
            "layout": "IPY_MODEL_51f3c17cef4e45abbbb5770fbac11dc5"
          }
        },
        "c69eedaad2354169aaf226b24101a030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d94bf32d9b410a830d6e2b33147e63",
            "placeholder": "​",
            "style": "IPY_MODEL_a956027ad1a8404d93cff0f21bc5c719",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "c5c61252502941aaa12389b8c9c49fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89714cecaa5641e487a3f85ea2535ddc",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_497f7d9d70134f05af4980dc5fc215cd",
            "value": 116
          }
        },
        "3b627e50ef294b2d9d7c012e17366a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c8776c2495c48f0a9ec09a34f918f4b",
            "placeholder": "​",
            "style": "IPY_MODEL_c26a0fac4d024c3fb4c48b484dae9186",
            "value": " 116/116 [00:00&lt;00:00, 8.19kB/s]"
          }
        },
        "51f3c17cef4e45abbbb5770fbac11dc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d94bf32d9b410a830d6e2b33147e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a956027ad1a8404d93cff0f21bc5c719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89714cecaa5641e487a3f85ea2535ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "497f7d9d70134f05af4980dc5fc215cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c8776c2495c48f0a9ec09a34f918f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26a0fac4d024c3fb4c48b484dae9186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba56dcd023a146938db1f2986f24cc70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_045f76061375436abc356c4dba19a391",
              "IPY_MODEL_15f85876a5ea450dbf14b57f107c85e5",
              "IPY_MODEL_0d6d3a75839f4a91821b4d032b5d4a5d"
            ],
            "layout": "IPY_MODEL_3a8683984e8247c4aaad0c65c4e09c4f"
          }
        },
        "045f76061375436abc356c4dba19a391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a7c18c407e405196fb866b6305f5b2",
            "placeholder": "​",
            "style": "IPY_MODEL_37442db8b99142fca2452221e66d0dc3",
            "value": "README.md: 100%"
          }
        },
        "15f85876a5ea450dbf14b57f107c85e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a1d3247cde4a69a15f7de7f733f076",
            "max": 10415,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_100f0cb787364302843c22420f4a934c",
            "value": 10415
          }
        },
        "0d6d3a75839f4a91821b4d032b5d4a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7458c0986884b1285ca5a17a1f772d9",
            "placeholder": "​",
            "style": "IPY_MODEL_3b0ebd40b4e94d2e926a2f9044896f3f",
            "value": " 10.4k/10.4k [00:00&lt;00:00, 564kB/s]"
          }
        },
        "3a8683984e8247c4aaad0c65c4e09c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a7c18c407e405196fb866b6305f5b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37442db8b99142fca2452221e66d0dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86a1d3247cde4a69a15f7de7f733f076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "100f0cb787364302843c22420f4a934c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7458c0986884b1285ca5a17a1f772d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0ebd40b4e94d2e926a2f9044896f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaffe9f50e3d4bc590270cd12dd0f5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e93ad7d8b2c4cceb38ec145a844681e",
              "IPY_MODEL_f39a1d826acc47ea81cfa06beecf18bf",
              "IPY_MODEL_73e167baf8974c8199e7e946a055eeef"
            ],
            "layout": "IPY_MODEL_ee116db04bd44a7ea911c21d15fba9e3"
          }
        },
        "8e93ad7d8b2c4cceb38ec145a844681e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_088496c66af545d994d882f26c8c6268",
            "placeholder": "​",
            "style": "IPY_MODEL_02e323735cca456eb33cf940b22c2ec8",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "f39a1d826acc47ea81cfa06beecf18bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_358d8b8b2bc049e58f97bab266d5f1db",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63ab437a6b3b47c1bbde331ad78523aa",
            "value": 53
          }
        },
        "73e167baf8974c8199e7e946a055eeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d25a7c34a8d14ffabab1ce1446162808",
            "placeholder": "​",
            "style": "IPY_MODEL_5daec1509e82477ca069876154d61ccd",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.48kB/s]"
          }
        },
        "ee116db04bd44a7ea911c21d15fba9e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "088496c66af545d994d882f26c8c6268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e323735cca456eb33cf940b22c2ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "358d8b8b2bc049e58f97bab266d5f1db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ab437a6b3b47c1bbde331ad78523aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d25a7c34a8d14ffabab1ce1446162808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5daec1509e82477ca069876154d61ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe32c2e8112d4d7d9796fd58b0934c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb7667dc93874acdaa632f598c25f94e",
              "IPY_MODEL_3cbc0b33999842849a89141a6f1be9e7",
              "IPY_MODEL_0ca7bcb04a6145d3ac59ccb84cce5f93"
            ],
            "layout": "IPY_MODEL_7ccd584f2c334810ac99f73ccb141a83"
          }
        },
        "bb7667dc93874acdaa632f598c25f94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cf34821e23e47deae0a79f1642f732b",
            "placeholder": "​",
            "style": "IPY_MODEL_562b031ec9de463db42593f58254ff65",
            "value": "config.json: 100%"
          }
        },
        "3cbc0b33999842849a89141a6f1be9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a4337814c54d269dec90fae07bc35f",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6228a8001944b50916f6317e9769c60",
            "value": 571
          }
        },
        "0ca7bcb04a6145d3ac59ccb84cce5f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84a5b3be41314c139b8cc25d160d89c0",
            "placeholder": "​",
            "style": "IPY_MODEL_812cd4c80ada4178917197b4d038abb7",
            "value": " 571/571 [00:00&lt;00:00, 45.2kB/s]"
          }
        },
        "7ccd584f2c334810ac99f73ccb141a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf34821e23e47deae0a79f1642f732b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562b031ec9de463db42593f58254ff65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76a4337814c54d269dec90fae07bc35f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6228a8001944b50916f6317e9769c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84a5b3be41314c139b8cc25d160d89c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812cd4c80ada4178917197b4d038abb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b29b5a9b5e204b0a8538e85299adec49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db71488537a64f3db3b75d87012ef394",
              "IPY_MODEL_8a349b1be5f74afe9225e632d2ec2da9",
              "IPY_MODEL_4ac39c5723a04c97aa1a42f9caf40c34"
            ],
            "layout": "IPY_MODEL_04d93de86f8947e284bf60d310d032f0"
          }
        },
        "db71488537a64f3db3b75d87012ef394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f22cc2f3309140578d5537edb69ad7be",
            "placeholder": "​",
            "style": "IPY_MODEL_45dbcff353424a75bd734252a5462742",
            "value": "model.safetensors: 100%"
          }
        },
        "8a349b1be5f74afe9225e632d2ec2da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f87df6825341988caf1162e50be566",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4320be6ca2bd424cbdb5b11535b53536",
            "value": 437971872
          }
        },
        "4ac39c5723a04c97aa1a42f9caf40c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31dad64db61347728ea9c5ed134e080e",
            "placeholder": "​",
            "style": "IPY_MODEL_c67c78f8f5514d909d72cbfe4341cc8e",
            "value": " 438M/438M [00:05&lt;00:00, 19.8MB/s]"
          }
        },
        "04d93de86f8947e284bf60d310d032f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22cc2f3309140578d5537edb69ad7be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45dbcff353424a75bd734252a5462742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55f87df6825341988caf1162e50be566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4320be6ca2bd424cbdb5b11535b53536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31dad64db61347728ea9c5ed134e080e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c67c78f8f5514d909d72cbfe4341cc8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c81ae6b3374842a7acd2f049901f77df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_196615141822478cabc7cbc786d0f0bb",
              "IPY_MODEL_58f9e11bb1ec4bc9865ef50203e1067f",
              "IPY_MODEL_6af298f9afd349c1b6985f80509450f1"
            ],
            "layout": "IPY_MODEL_90cbc11eb07b4345be51a5fe0f86674d"
          }
        },
        "196615141822478cabc7cbc786d0f0bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2246c3a09e641ba829b8d0e257f8bd5",
            "placeholder": "​",
            "style": "IPY_MODEL_260137ffb9544dca84543e1a7140a983",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "58f9e11bb1ec4bc9865ef50203e1067f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56ec1ab838d4b34bf010b64722f88a3",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3be028e1883d44afbe2fd6b17560147e",
            "value": 363
          }
        },
        "6af298f9afd349c1b6985f80509450f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99a80fba12674bbc8ead54a6e8112243",
            "placeholder": "​",
            "style": "IPY_MODEL_343b206150a144e1b956f86886a585de",
            "value": " 363/363 [00:00&lt;00:00, 19.9kB/s]"
          }
        },
        "90cbc11eb07b4345be51a5fe0f86674d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2246c3a09e641ba829b8d0e257f8bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "260137ffb9544dca84543e1a7140a983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d56ec1ab838d4b34bf010b64722f88a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be028e1883d44afbe2fd6b17560147e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99a80fba12674bbc8ead54a6e8112243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "343b206150a144e1b956f86886a585de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f2c4e82e2b6404099ceab5c1b51f884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5fec214606f4ebf89271f145410addf",
              "IPY_MODEL_9050e8d5fa564b609b33e6cdb8866e9d",
              "IPY_MODEL_c1f229c767744f8ea62d13918cdc144e"
            ],
            "layout": "IPY_MODEL_d000f251a61d435b81a8fd6e5f7c74e0"
          }
        },
        "a5fec214606f4ebf89271f145410addf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa7fa12962404d859aaab9f1a3b322d0",
            "placeholder": "​",
            "style": "IPY_MODEL_b4136b0d4d19457c8bab81c6bb19f3cc",
            "value": "vocab.txt: 100%"
          }
        },
        "9050e8d5fa564b609b33e6cdb8866e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29e82b9d3bb146aa973199ebef3061dd",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c65577cd8fa64ca0a848ede78ab230b0",
            "value": 231536
          }
        },
        "c1f229c767744f8ea62d13918cdc144e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5183b3130d8c443aa91dac212c9840b7",
            "placeholder": "​",
            "style": "IPY_MODEL_4896f1e6140340268bd8dbb5e090345d",
            "value": " 232k/232k [00:00&lt;00:00, 8.42MB/s]"
          }
        },
        "d000f251a61d435b81a8fd6e5f7c74e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7fa12962404d859aaab9f1a3b322d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4136b0d4d19457c8bab81c6bb19f3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29e82b9d3bb146aa973199ebef3061dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c65577cd8fa64ca0a848ede78ab230b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5183b3130d8c443aa91dac212c9840b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4896f1e6140340268bd8dbb5e090345d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79b7c46d7640411998d25fcd849930dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac3a40d698674fbfb459fbf0cfd80932",
              "IPY_MODEL_638a056fe8e140b68c006bd25800ecaf",
              "IPY_MODEL_48bac1d81bcf4e56b2d49497b12e2f78"
            ],
            "layout": "IPY_MODEL_170b5f6054cb40eb8d17a4852709149f"
          }
        },
        "ac3a40d698674fbfb459fbf0cfd80932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82bb2aaa2dc14ff692c6ba89cfd41bbb",
            "placeholder": "​",
            "style": "IPY_MODEL_651ef583dd704cf3b8ad65643e8d8bf3",
            "value": "tokenizer.json: 100%"
          }
        },
        "638a056fe8e140b68c006bd25800ecaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ffeafd64174bcfb4c9e2741bd8f455",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c334c60e012540f0acde256473bf0d6f",
            "value": 466021
          }
        },
        "48bac1d81bcf4e56b2d49497b12e2f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43fc772f91d142cfa082af87231890f1",
            "placeholder": "​",
            "style": "IPY_MODEL_c4abeb7f21fe4f5e9b4e2b495deae46e",
            "value": " 466k/466k [00:00&lt;00:00, 13.7MB/s]"
          }
        },
        "170b5f6054cb40eb8d17a4852709149f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82bb2aaa2dc14ff692c6ba89cfd41bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "651ef583dd704cf3b8ad65643e8d8bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07ffeafd64174bcfb4c9e2741bd8f455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c334c60e012540f0acde256473bf0d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43fc772f91d142cfa082af87231890f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4abeb7f21fe4f5e9b4e2b495deae46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2530a2ea0c5840a091cb9dc6950119ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dfd93b82f204129b8c50e487734c29c",
              "IPY_MODEL_699e48af32304d7eafba2b59a58e8023",
              "IPY_MODEL_c7ff257844f7415e8c159be8e9826232"
            ],
            "layout": "IPY_MODEL_041b7ab3e31f4c0a8af005fcd8534583"
          }
        },
        "1dfd93b82f204129b8c50e487734c29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e83baac3a4341c1b494b0d1855f2e91",
            "placeholder": "​",
            "style": "IPY_MODEL_810b65589af047f6bb6a59d776e0c1a4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "699e48af32304d7eafba2b59a58e8023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee4fd55aebd4ca6978d6a91d0560150",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98e31ca0e9124b7dbc758bf4cdaa6815",
            "value": 239
          }
        },
        "c7ff257844f7415e8c159be8e9826232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb855e406c840f39f2ec5a4689509eb",
            "placeholder": "​",
            "style": "IPY_MODEL_4166abdd4a974f7f990f953b35329251",
            "value": " 239/239 [00:00&lt;00:00, 19.7kB/s]"
          }
        },
        "041b7ab3e31f4c0a8af005fcd8534583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e83baac3a4341c1b494b0d1855f2e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "810b65589af047f6bb6a59d776e0c1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ee4fd55aebd4ca6978d6a91d0560150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e31ca0e9124b7dbc758bf4cdaa6815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fb855e406c840f39f2ec5a4689509eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4166abdd4a974f7f990f953b35329251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af077b37d82e445faeb6415d7da98a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95b19a012ef249ebb7f8fc584eb1bd05",
              "IPY_MODEL_094dd4153aaa49aaba6f9730d9c861bc",
              "IPY_MODEL_9ba4635f5a9d47a892dfcbc91e43cd25"
            ],
            "layout": "IPY_MODEL_aec6c48ab9104331abeacdb735abe923"
          }
        },
        "95b19a012ef249ebb7f8fc584eb1bd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e5beb3071f44b33866b140d2e26c362",
            "placeholder": "​",
            "style": "IPY_MODEL_22c3353318f74e64b89b11020fb4fec4",
            "value": "config.json: 100%"
          }
        },
        "094dd4153aaa49aaba6f9730d9c861bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f6cb19daeb6403db5e1b93fcedf9037",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d936f81671e41f4be8e3c4e627a2ce4",
            "value": 190
          }
        },
        "9ba4635f5a9d47a892dfcbc91e43cd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6c2d39efa8b49a1b252e857c3ab4efb",
            "placeholder": "​",
            "style": "IPY_MODEL_83475b75766a4251850d94110b4eefb7",
            "value": " 190/190 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "aec6c48ab9104331abeacdb735abe923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e5beb3071f44b33866b140d2e26c362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22c3353318f74e64b89b11020fb4fec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f6cb19daeb6403db5e1b93fcedf9037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d936f81671e41f4be8e3c4e627a2ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6c2d39efa8b49a1b252e857c3ab4efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83475b75766a4251850d94110b4eefb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terrytaylorbonn/365_langchain_rabbit/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing libraries and connect to LLMs"
      ],
      "metadata": {
        "id": "pNH6I4yNTvxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W2OZYI5XlXXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58263be6-9540-464f-c9d3-fefbf6ea5986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/644.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.4/644.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU  \\\n",
        "  python-dotenv \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  openai \\\n",
        "  anthropic \\\n",
        "  langchain-openai \\\n",
        "  langchain-anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the API keys from the .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZbN7ti31Adv",
        "outputId": "7bc4b3e1-9007-4fa2-dd8a-81f25242cfbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to OpenAI and Anthropic\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm_claude3 = ChatAnthropic(model='claude-3-opus-20240229')\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm_gpt4 = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "j8K_zG4D1HqA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that you can use the LLM\n",
        "llm_claude3.invoke(\"What is LangChain?\").content"
      ],
      "metadata": {
        "id": "db48f5Ei1KSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "c40e654c-2628-49af-cecd-9e5954eef5ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain is an open-source framework designed to assist in the development of applications powered by language models. It provides a set of tools and modules that make it easier to build scalable and maintainable applications that leverage the power of large language models (LLMs) like OpenAI's GPT-3.\\n\\nKey features of LangChain include:\\n\\n1. Modular components: LangChain offers a variety of modular components that can be combined and customized to suit specific use cases. These components include prompts, parsers, memory, and agents.\\n\\n2. Support for multiple LLMs: LangChain is compatible with various LLMs, including OpenAI's models, Hugging Face's Transformers, and more. This allows developers to choose the most suitable language model for their application.\\n\\n3. Chain management: LangChain simplifies the process of chaining together multiple components, such as prompts, parsers, and memory, to create complex language model applications.\\n\\n4. Memory management: LangChain provides tools for managing conversation history and context, enabling more coherent and contextually relevant interactions with language models.\\n\\n5. Extensibility: Developers can easily extend LangChain's functionality by creating custom components or integrating existing tools and libraries.\\n\\nLangChain aims to streamline the development process and provide a structured approach to building applications that utilize language models effectively. It is particularly useful for tasks such as chatbots, question-answering systems, text summarization, and more.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that you can use the LLM\n",
        "llm_gpt4.invoke(\"What is LangChain?\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "Z6kHSVM28B1F",
        "outputId": "92c40510-6c1f-4c31-bbf2-032060676248"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an open-source framework designed to help developers build applications that leverage large language models (LLMs). It provides a structured approach to integrating LLMs with other data sources and workflows. LangChain is especially useful for creating applications that require complex interactions with text data, such as chatbots, text analysis tools, and question-answering systems.\\n\\nThe framework focuses on key areas such as:\\n\\n1. **Prompt Management**: Facilitating the efficient creation and management of prompts, which are crucial for guiding the behavior of language models.\\n\\n2. **Chaining**: Allowing developers to create sequences or chains of calls to language models and custom logic, enabling more complex interactions and responses.\\n\\n3. **Indexing and Retrieval**: Providing tools to build and use indexes over document collections, enabling enhanced search and retrieval capabilities.\\n\\n4. **APIs and Integrations**: Offering built-in integrations with various APIs and data sources to expand the capabilities of language models beyond what they can achieve independently.\\n\\n5. **Memory Management**: Enabling applications to maintain conversational memory, which is essential for creating more natural and coherent interactions with users over time.\\n\\nOverall, LangChain is utilized by developers looking to simplify the integration of LLMs into their applications, reduce development time, and create sophisticated language-based applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic request using system and human/user message\n",
        "\n",
        "system_prompt=\"\"\"\n",
        "You explain things to people like they are five year olds.\n",
        "\"\"\"\n",
        "user_prompt=f\"\"\"\n",
        "What is LangChain?\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import textwrap\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_prompt),\n",
        "]"
      ],
      "metadata": {
        "id": "sjfYGNmd1OfD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm_gpt4.invoke(messages)\n",
        "answer = textwrap.fill(response.content, width=100)"
      ],
      "metadata": {
        "id": "fIOYvhTR1u2X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "id": "tulS9RsH1r86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f039a9e2-3f73-438e-a8a6-d0a38ec0558b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alright, imagine you have a big box of Lego blocks, and each block can do something cool. Some can\n",
            "make sounds, some can light up, and others can move things around. But what if you want to build\n",
            "something amazing that uses all these different blocks together? That's where LangChain comes in.\n",
            "LangChain is like a super smart guide that helps you put all these Lego blocks together in a special\n",
            "way. Instead of Lego blocks, LangChain helps connect different parts of computer programs that work\n",
            "with language. It's like building a cool robot that can understand and talk to people, using\n",
            "different pieces that each do something unique.  So, LangChain helps people create smart computer\n",
            "programs that can read, write, and understand human language by connecting different pieces, just\n",
            "like building with Lego!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm_claude3.invoke(messages)\n",
        "answer = textwrap.fill(response.content, width=100)"
      ],
      "metadata": {
        "id": "lADKsC7F8mWc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfWW32-z8wse",
        "outputId": "54aef868-79ba-4280-aafa-e431ef8e570c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a tool that helps computers talk to each other better. You know how sometimes when you\n",
            "play with your friends, you have to explain the rules of the game so everyone understands? LangChain\n",
            "does something similar for computers.  It's like a special language that computers use to share\n",
            "information and work together on tasks. With LangChain, different computer programs can easily\n",
            "connect and understand each other, just like how you and your friends can play together nicely when\n",
            "you all know the rules.  This makes it easier for people who make computer programs to build cool\n",
            "things, like games or apps that can do lots of different tasks. LangChain helps the different parts\n",
            "of the program work together smoothly, just like how all the pieces of a puzzle fit together to make\n",
            "a nice picture.  So, in simple words, LangChain is a helpful tool that lets computers communicate\n",
            "and cooperate better, making it easier for people to create awesome programs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chains, Prompts and Loaders"
      ],
      "metadata": {
        "id": "jFugvpEcTpdR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGF_96Sq8kcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "mhgtPPEcN_dJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple prompt template\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following input:\n",
        "{topic}\n",
        "Provide an explanation of the given topic.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt from the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "7NZqTnzIOL0_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the chain using the pipe operator \"|\", more on that later\n",
        "chain = prompt | llm_gpt4"
      ],
      "metadata": {
        "id": "PDwGnPSeP_q3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\":\"What is LangChain\"}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "Wcb1W45pQEFJ",
        "outputId": "fc308e48-68cd-42f7-e617-928affa1bc46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain is an open-source framework designed to help developers build applications that can interact with language models more efficiently and effectively. It offers tools and features that simplify the integration and use of large language models (LLMs) like GPT-3 and GPT-4 into various applications. \\n\\nLangChain focuses on several core areas:\\n\\n1. **Chain of Thought (CoT):** LangChain enables the use of chain of thought reasoning, which allows AI models to handle more complex tasks by breaking down the thought process into structured steps. This makes the language model's decision-making process more transparent and manageable.\\n\\n2. **Composability:** The framework allows developers to compose and manage sequences of interactions with language models, making it easier to build complex applications that require multiple steps or stages of processing.\\n\\n3. **Integration:** LangChain facilitates the integration of language models into existing software ecosystems. It provides interfaces and components that allow developers to connect language models with other data sources and systems seamlessly.\\n\\n4. **Prompt Management:** One of the key features of LangChain is its ability to manage prompts effectively. It helps in designing, deploying, and managing prompts to ensure that interactions with LLMs are consistent and produce the desired outputs.\\n\\n5. **Scaling and Optimization:** LangChain provides tools for optimizing the performance and scalability of language model applications, helping developers to deploy solutions that can handle larger loads and more complex tasks efficiently.\\n\\nOverall, LangChain aims to make it easier for developers to harness the capabilities of large language models, enabling them to build sophisticated AI-powered applications that require natural language understanding and generation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  youtube-transcript-api"
      ],
      "metadata": {
        "id": "a6L3kvBFRasB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108017b6-4a03-4539-c3ee-06e8ad77c2a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Youtube Loader from the LangChain community\n",
        "\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
        ")"
      ],
      "metadata": {
        "id": "FTkY3wQAOztq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the video transcript as documents\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "UuclzSCERocY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwFI1F1hneGi",
        "outputId": "ce68be15-79cc-4d92-9f7b-5549a15b369d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\")]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript=docs[0].page_content"
      ],
      "metadata": {
        "id": "4RWjlF2RRrFE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now use the transcript in a chain\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
        "{video_transcript}\n",
        "Give a summary.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"video_transcript\"],\n",
        "    template=prompt_template,\n",
        ")"
      ],
      "metadata": {
        "id": "kjtcIV1mR1FK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_gpt4"
      ],
      "metadata": {
        "id": "jXALdqNiSP3S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that we can just feed the chain the docs without extracting the content as text\n",
        "\n",
        "chain.invoke({\"video_transcript\":docs}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "_6RbQR5ESUDR",
        "outputId": "c3e45a4b-35df-4ca9-ef94-c722fe652afb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The video discusses the capabilities and applications of Lama 3, a language model developed by Meta, for data analysis using language model-based SQL generation. The presenter demonstrates how Lama 3, hosted on Gro Cloud, can produce advanced SQL queries quickly. They explain how to enhance the performance of SQL chains by implementing a self-correcting mechanism that feeds error messages back into the chain, enabling retries until successful execution.\\n\\nThe video provides a step-by-step guide on setting up the environment to use Lama 3, including using Gro Cloud, Lang Chain, and other required libraries. The speaker demonstrates extracting schema information from a dataset and connecting to Lama 3 to generate SQL code from natural language prompts. They highlight challenges in getting executable SQL and show how a regex function can extract the correct code from the model's response.\\n\\nSeveral examples are given, including generating a list of top customers and revenue by acquisition channel. They show how the error feedback mechanism improves accuracy and execution success over multiple attempts. Finally, the presenter comments on the implications of using Lama 3 for insights generation, highlighting its benefits for privacy-sensitive data and query-heavy applications and predicting that LLMs will increasingly drive data-related tasks in the future.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "14-68lwFSsxI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The create_stuff_documents_chain takes a list of docs and formats them all into a prompt\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following context:\n",
        "{context}\n",
        "Summarize what Llama 3 can do.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "chain = create_stuff_documents_chain(llm_gpt4, prompt)"
      ],
      "metadata": {
        "id": "OYI-Ej1DSw9j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs"
      ],
      "metadata": {
        "id": "vKAUFQElTPHh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"context\": docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "Lb_RBPa2TSNf",
        "outputId": "c8587be6-cfa2-405f-b754-77c73f483d14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Llama 3, developed by Meta, is a powerful language model with significant capabilities in data analysis using natural language processing. Here are the main functions and features of Llama 3 as outlined in the context:\\n\\n1. **Text to SQL Generation**: Llama 3 can convert natural language queries into SQL queries. It can generate advanced SQL almost on par with high-IQ language models when a specific tweak is implemented.\\n\\n2. **Self-Correcting SQL Chains**: By utilizing error messages, the SQL chains can be made self-correcting. If an error occurs during SQL execution, the error is fed back into the chain to refine the query until it is successful.\\n\\n3. **Insight Generation**: Llama 3 is capable of generating insights from data sets. It can handle various queries such as listing the best customers, breaking down revenue by acquisition channel, and identifying top customers by purchase frequency.\\n\\n4. **Privacy Considerations**: As an open-source model, Llama 3 is beneficial for privacy-sensitive use cases, allowing for localized deployment to handle sensitive data without relying on external cloud services.\\n\\n5. **Efficiency for Query-Heavy Applications**: Due to Gro Cloud's real-time inference capabilities, Llama 3 is efficient for consumer-facing applications and query-heavy use cases, making it suitable for environments where speed is crucial.\\n\\n6. **Future of Data Pipelines**: The use of Llama 3 suggests a trend towards AI-generated data pipelines, dashboards, and reports, indicating a shift in how data analysis will be approached in the future.\\n\\nOverall, Llama 3 presents a significant advancement in the application of AI for data analysis, particularly in making SQL generation more efficient and accessible.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. LCEL & Runnables"
      ],
      "metadata": {
        "id": "HDpbfZb1TknL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "summarize_prompt_template = \"\"\"\n",
        "You are a helpful assistant that summarizes AI concepts:\n",
        "{context}\n",
        "Summarize the context\n",
        "\"\"\"\n",
        "\n",
        "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)\n"
      ],
      "metadata": {
        "id": "HhaQozKlrnkC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "848txAD_o8kR",
        "outputId": "1932ea94-d0ed-4039-bd4b-25aed8b860fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant that summarizes AI concepts:\\n{context}\\nSummarize the context\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Chain with the \"|\" operator"
      ],
      "metadata": {
        "id": "r264yXoiv7-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "NPx6Qz_bTiwz",
        "outputId": "9e047a7e-b29c-4464-d1cd-319c78b45fb3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a framework specifically designed for developing applications powered by language models. It provides a suite of tools and abstractions that simplify the process of building applications that rely on large language models, like GPT-3, by offering features such as:\\n\\n1. **Chain-of-Thought Prompting**: LangChain allows developers to create more complex sequences of thought and logic by linking multiple prompts together, facilitating the creation of more sophisticated and nuanced interactions.\\n\\n2. **Memory and Context Handling**: The framework supports handling of memory, enabling applications to maintain context across interactions. This means a chatbot or AI application can remember previous parts of a conversation or session, making interactions more coherent and personalized.\\n\\n3. **Integration with External Tools**: LangChain makes it easier to integrate language models with other tools and data sources, enhancing their functionality by allowing them to pull in and process information from various external systems.\\n\\n4. **Flexibility and Modularity**: The framework is designed to be flexible and modular, allowing developers to customize and extend it according to their specific needs.\\n\\nOverall, LangChain aims to streamline the development process for applications that leverage language models, helping developers to more easily harness the power of these models for a wide range of use cases.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the type of the chain\n",
        "print(type(chain)) # Should print <class 'langchain_core.runnables.base.RunnableSequence'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW3dgnfXKU8r",
        "outputId": "f7f54f9b-95df-48ce-dc94-6492e15a6cdd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a RunnableLambda"
      ],
      "metadata": {
        "id": "ke-ycihtrxpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject python functions into a chain with RunnableLambda\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Define a custom lambda function and wrap it in RunnableLambda\n",
        "length_lambda = RunnableLambda(lambda summary: f\"Summary length: {len(summary)} characters\")\n",
        "\n",
        "lambda_chain = summarize_chain | length_lambda\n",
        "\n",
        "lambda_chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "id": "GEs1q_b6Kgs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5fe869e0-81ae-43a0-94b9-156b7453b512"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 1444 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(lambda_chain.steps[-1])) # Should print <class 'langchain_core.runnables.base.RunnableLambda'>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-OhiwW33iiU",
        "outputId": "d73830a9-f4b8-43c1-ca92-c92841dad0fa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use function in chain without converting to RunnableLambda\n",
        "chain_with_function = summarize_chain |  (lambda summary: f\"Summary length: {len(summary)} characters\")"
      ],
      "metadata": {
        "id": "JI2Y4s7oUYOI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(chain_with_function.steps[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZgqYfgj3hS1",
        "outputId": "55f94f64-03df-4202-a442-6cb218efcb80"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_function.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q3x7H60speRq",
        "outputId": "dc72ec8d-f366-45b8-90bf-683b018666d8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 825 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnablePassthrough as placeholder"
      ],
      "metadata": {
        "id": "75J0BqR3uKpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Create a RunnablePassthrough instance\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Create the sequence using the pipe operator with summarization and length calculation\n",
        "placeholder_chain = summarize_chain| passthrough | length_lambda\n",
        "\n",
        "placeholder_chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "p6rAREimtqLi",
        "outputId": "164795ba-15e8-4bed-8b40-f0b32718f4c3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 567 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(placeholder_chain.steps[-1]))  # Should print <class 'langchain_core.runnables.base.RunnableLambda'>\n",
        "print(type(placeholder_chain.steps[-2]))  # Should print <class 'langchain_core.runnables.passthrough.RunnablePassthrough'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eZz4CEpviLa",
        "outputId": "cee16ee9-30a2-4680-b95d-97f96b274226"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
            "<class 'langchain_core.runnables.passthrough.RunnablePassthrough'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnablePassthrough for assignment"
      ],
      "metadata": {
        "id": "Cc8VpIM4uQxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom lambda function to wrap the summary in a dictionary\n",
        "wrap_summary_lambda = RunnableLambda(lambda summary: {\"summary\": summary})\n",
        "\n",
        "# Create a RunnablePassthrough instance that assigns additional information\n",
        "assign_passthrough = RunnablePassthrough.assign(length=lambda x: len(x[\"summary\"]))\n",
        "\n",
        "# Create the summarization chain\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser | wrap_summary_lambda\n",
        "\n",
        "# Create the full chain combining summarization and assign_passthrough\n",
        "assign_chain = summarize_chain | assign_passthrough\n",
        "\n",
        "# Use the chain\n",
        "assign_chain.invoke({\"context\": \"What is LangChain?\"})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITVOr1PJ9u5z",
        "outputId": "a9c3d911-a385-4e8e-bfa8-43050eb164ac"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'LangChain is an open-source framework designed to help developers create applications powered by large language models (LLMs). Its primary focus is to assist in building more sophisticated and capable language model-powered applications by not only interacting with a single LLM but also by allowing composition over several different components. This includes the capabilities to manage and use memory, interact with data stores, utilize multiple LLMs in sequence or parallel, and integrate with external APIs.\\n\\nLangChain provides modularized components that allow users to customize and extend functionalities as needed, such as for information retrieval, question answering, and decision-making. It is geared towards enabling the development of cutting-edge natural language processing applications that can dynamically respond to complex tasks, with use cases ranging from chatbot creation to more extensive data analysis and automation workflows.',\n",
              " 'length': 951}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(assign_chain.steps[-1])) # Should print <class 'langchain_core.runnables.passthrough.RunnableAssign'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilA-25jYaITV",
        "outputId": "23fcb959-8dee-41a0-f81b-a7d9bc561bbc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.passthrough.RunnableAssign'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using RunnableParallel"
      ],
      "metadata": {
        "id": "XhRfKg_Wws9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# Create the summarization chain\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Create a RunnableParallel instance to handle summary and length in parallel\n",
        "parallel_runnable = RunnableParallel(\n",
        "    summary=lambda x: x,  # Passes the summary as is\n",
        "    length=lambda x: len(x)  # Calculates the length of the summary\n",
        ")\n",
        "\n",
        "# Combine the summarization chain with parallel runnable\n",
        "parallel_chain = summarize_chain | parallel_runnable\n",
        "\n",
        "parallel_chain.invoke({\"context\": \"What is LangChain?\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwgwh2Czzx75",
        "outputId": "f628b367-a3b9-4243-a643-6f268db8cce0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': \"LangChain is a framework designed to facilitate the development of applications that leverage large language models (LLMs). It provides tools and abstractions to help integrate language models with other components and data sources, enabling developers to build applications for various tasks such as text generation, question answering, and more. LangChain focuses on the following areas:\\n\\n1. **LLM Chains**: It provides a way to chain together different operations involving language models. These chains can include multiple steps, such as querying an LLM, processing the output, and feeding it into another model or function.\\n\\n2. **Data Connection and Management**: LangChain helps manage interactions between LLMs and external data sources. This includes connecting to databases, APIs, and other input/output systems to provide context and enrich the model's responses.\\n\\n3. **Memory**: The framework allows for incorporating memory into LLM-driven applications. This means applications can have a stateful context, which can improve interactions by maintaining context over a series of exchanges.\\n\\n4. **Agents**: LangChain facilitates the creation of agents, which are systems that can autonomously interact with the world by making decisions and taking actions based on the language model's outputs.\\n\\n5. **Evaluation and Experimentation**: The framework aids in evaluating language model outputs and experimenting with different configurations to improve the performance and reliability of applications.\\n\\nOverall, LangChain aims to simplify the process of building complex, feature-rich applications utilizing the capabilities of large language models by providing a structured way to combine model outputs with other software components.\",\n",
              " 'length': 1744}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the type of the last element in the chain\n",
        "print(type(parallel_chain.steps[-1]))  # Should print <class 'langchain_core.runnables.parallel.RunnableParallel'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MSV99HovG1f",
        "outputId": "9e2936a3-f699-4eb1-88e4-09016c4c022b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableParallel'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Retrievers & Splitters"
      ],
      "metadata": {
        "id": "2xnloFfrKdVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  redis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtV7hn48HkHG",
        "outputId": "6903ba64-54c8-4a8a-d7e6-545173d0bab4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/261.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Youtube Loader from the LangChain community\n",
        "\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
        ")\n",
        "\n",
        "# Load the video transcript as documents\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "F-16JiI-w-y0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9oNaHe89j9j",
        "outputId": "e8c82407-f116-4119-e7e1-203cdb681cf3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\")]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "-8_5WwpHDoFi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "EMfIZb4YDoQJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "FkKhVCMcDoXX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFpE66iXDods",
        "outputId": "f85e3036-0ce6-4d40-bd6a-3e367ecbf09c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='can read about Lama 3 and some of the capabilities of the model you can see how the model compares'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"the model compares to other popular llms specifically the 70b model that I'm going to be using in\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"chains with L chain all right the first thing I'm going to do is I'm going to set up the cop\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to set up the cop notebook and pip install the required libraries then I'll connect to Bay and\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"connect to Bay and fetch the schema information from tables in a data set I'll be installing\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='have two functions that extracts the schema information from bit query in a schema. py file and'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"schema. py file and this is the same functions that I've been using in the earlier videos that lets\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='videos that lets me extract and feed the scheme information from bitre to the chain all right so'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"chain all right so I'm just going to load the environment variables and then I'm going to connect\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"going to connect to biy and the data set I'm using is the same data set I used in the last video in\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the last video in the dashboard video it is an e-commerce data set with four tables customers'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='tables customers orders products and customer taxs and the two functions in the schema. py file'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the schema. py file allows me to extract the schema information from the data set as you can see'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"model name in this case I'm using Lama 370b and the prom template will be injecting three things\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"three things first I'll inject the schema information I'm extracting from the bit crate data set\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"bit crate data set then I'm injecting the main question or the query and finally I'll be injecting\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'll be injecting a a message history and the message history is The Tweak I mentioned in the\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'm going to catch the error and feed it back to the chain and this allows me to make the chains\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to make the chains self-correcting which is useful when we're dealing with a model that is of a low\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='path through to to inject the schema information and to inject the messages of the message history'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the message history that contains the errors then I use my prompt the language model and a string'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='model and a string output passer and this is what we need to generate the SQL code from a prompt'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"code from a prompt now let's move on to generate some insights with this setup I had difficulties\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"function that lets me extract the SQL code from the response and in this extract SQL function I'm\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"SQL function I'm simply using regex to extract the SQL from whatever the language model is\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"language model is returning to wrap it all up I'm creating a function that takes a prompt and a\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"there's an error I'm going to collect the error and feed it back to the chain and try again and in\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='try again and in this way the chain will be self-corrected ing because the llm will understand the'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"will understand the error message okay so let's try this the first prompt I'm going to give L three\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='to give L three is the following give me a list of the best customers including their rank their'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='their rank their first name last name and email and the products they purchased and this one it got'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='and this one it got in the first attempt so the query executed successfully and we can then have a'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='we can then have a look at the data frame and this is essentially an audience that you could use'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='that you could use for marketing purposes you normally create an audience like this in a customer'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"this in a customer data platform now let's try a different one I'll do a classical one show me the\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='one show me the revenue generated in the last 30 days broken down by acquisition Channel and here'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='feeding back the error message makes the second attempt successful and here we have Revenue broken'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"have Revenue broken down by acquisition source let's do another audience let's say I want the top\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='say I want the top 100 customers with the highest purchase frequency but with an under average aov'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='under average aov and again we see that the first attempt fails and the second attempt is'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"second attempt is successful and here we only got the names let's say that we want to include the\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='want to include the frequency and the aov as well to get the full overview and here we see that the'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='we see that the first attempt fails the second attempt also fails but the third attempt is a'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='third attempt is a success and here we have the full audience data frame with the purchase'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='with the purchase frequency and the average order value so this is very useful so we can use llama'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='so we can use llama 3 for generating insights if we just implement this small tweak of catching the'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='of catching the errors and feeding it back to the chain all right so what are the implications of'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the implications of this first of all we now know that with Lama 3 open source llms can be used to'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='llms can be used to generate insights and this is very good news for privacy sensitive use cases so'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='use cases so use cases where you want to feed sensitive customer data back to the llm and in real'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='you want to use Lama 3 locally this is also very good news for query heavy applications so text'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"so text tosql applications can be query heavy if they're rolled out in a big organization so\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"big organization so there's a cost consideration that might be worth looking into now Gro cloud is\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='going to be really useful for Consumer facing applications where speed is necessary finally I think'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='so on will be llm generated in the future and not so distant future so if you are a data analyst or'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='a data analyst or data engineer you should really pay attention to this and learn this new'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"and learn this new technology all right that's it for now if you enjoyed this video I suggest you\"),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='video I suggest you check out one of the other videos on generating SQL with llm chains thanks for'),\n",
              " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='chains thanks for watching')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REDIS_URL=\"redis://default:nk5IAivbxEq0bnpi72GvBh21AeenDkDZ@redis-15360.c239.us-east-1-2.ec2.redns.redis-cloud.com:15360\"\n",
        "REDIS_HOST=\"redis-15360.c239.us-east-1-2.ec2.redns.redis-cloud.com\"\n",
        "REDIS_PASSWORD=\"nk5IAivbxEq0bnpi72GvBh21AeenDkDZ\"\n",
        "REDIS_PORT=\"15360\"\n"
      ],
      "metadata": {
        "id": "5yQG7bkSHBGl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "r = redis.Redis(\n",
        "  host=REDIS_HOST,\n",
        "  port=REDIS_PORT,\n",
        "  password=REDIS_PASSWORD)"
      ],
      "metadata": {
        "id": "sZoU-4ucDojx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.ping()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U_pMl4WDopo",
        "outputId": "013c436a-be0e-4120-ed28-83f5f483cb4c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.flushdb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ACoTRLuL5CU",
        "outputId": "031678f6-1694-4a90-8d65-cb01f6d33187"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls9EpiPnyNJ4",
        "outputId": "662763bf-1a86-46b9-c59d-b859f1df822f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606,
          "referenced_widgets": [
            "2ae9b5dd07f04243938a057bb5ca02a1",
            "c53774b5d09d4ae480def47682b65adb",
            "b6ae4ad6a11147e081a14480c638ed9b",
            "5aa838a2059f455fab7e50341934bcb3",
            "8e9238bc42734ad9a6e5c831a54bdcd6",
            "b6792a2717af4c0b9644f633af2c53b5",
            "76bdf2be51b144e3960b7c2e82d20940",
            "ca708d891012453ea7dd04cc0cb1fab8",
            "07acb5912ef04ebda8dfb3c282d9011b",
            "b8a303daef8a4369a9fd18424dadf7b7",
            "e1e31a3507034159b90823c16940164c",
            "aced000579ac473988356c0228a85951",
            "c69eedaad2354169aaf226b24101a030",
            "c5c61252502941aaa12389b8c9c49fb4",
            "3b627e50ef294b2d9d7c012e17366a31",
            "51f3c17cef4e45abbbb5770fbac11dc5",
            "18d94bf32d9b410a830d6e2b33147e63",
            "a956027ad1a8404d93cff0f21bc5c719",
            "89714cecaa5641e487a3f85ea2535ddc",
            "497f7d9d70134f05af4980dc5fc215cd",
            "2c8776c2495c48f0a9ec09a34f918f4b",
            "c26a0fac4d024c3fb4c48b484dae9186",
            "ba56dcd023a146938db1f2986f24cc70",
            "045f76061375436abc356c4dba19a391",
            "15f85876a5ea450dbf14b57f107c85e5",
            "0d6d3a75839f4a91821b4d032b5d4a5d",
            "3a8683984e8247c4aaad0c65c4e09c4f",
            "f6a7c18c407e405196fb866b6305f5b2",
            "37442db8b99142fca2452221e66d0dc3",
            "86a1d3247cde4a69a15f7de7f733f076",
            "100f0cb787364302843c22420f4a934c",
            "b7458c0986884b1285ca5a17a1f772d9",
            "3b0ebd40b4e94d2e926a2f9044896f3f",
            "eaffe9f50e3d4bc590270cd12dd0f5b1",
            "8e93ad7d8b2c4cceb38ec145a844681e",
            "f39a1d826acc47ea81cfa06beecf18bf",
            "73e167baf8974c8199e7e946a055eeef",
            "ee116db04bd44a7ea911c21d15fba9e3",
            "088496c66af545d994d882f26c8c6268",
            "02e323735cca456eb33cf940b22c2ec8",
            "358d8b8b2bc049e58f97bab266d5f1db",
            "63ab437a6b3b47c1bbde331ad78523aa",
            "d25a7c34a8d14ffabab1ce1446162808",
            "5daec1509e82477ca069876154d61ccd",
            "fe32c2e8112d4d7d9796fd58b0934c89",
            "bb7667dc93874acdaa632f598c25f94e",
            "3cbc0b33999842849a89141a6f1be9e7",
            "0ca7bcb04a6145d3ac59ccb84cce5f93",
            "7ccd584f2c334810ac99f73ccb141a83",
            "4cf34821e23e47deae0a79f1642f732b",
            "562b031ec9de463db42593f58254ff65",
            "76a4337814c54d269dec90fae07bc35f",
            "e6228a8001944b50916f6317e9769c60",
            "84a5b3be41314c139b8cc25d160d89c0",
            "812cd4c80ada4178917197b4d038abb7",
            "b29b5a9b5e204b0a8538e85299adec49",
            "db71488537a64f3db3b75d87012ef394",
            "8a349b1be5f74afe9225e632d2ec2da9",
            "4ac39c5723a04c97aa1a42f9caf40c34",
            "04d93de86f8947e284bf60d310d032f0",
            "f22cc2f3309140578d5537edb69ad7be",
            "45dbcff353424a75bd734252a5462742",
            "55f87df6825341988caf1162e50be566",
            "4320be6ca2bd424cbdb5b11535b53536",
            "31dad64db61347728ea9c5ed134e080e",
            "c67c78f8f5514d909d72cbfe4341cc8e",
            "c81ae6b3374842a7acd2f049901f77df",
            "196615141822478cabc7cbc786d0f0bb",
            "58f9e11bb1ec4bc9865ef50203e1067f",
            "6af298f9afd349c1b6985f80509450f1",
            "90cbc11eb07b4345be51a5fe0f86674d",
            "a2246c3a09e641ba829b8d0e257f8bd5",
            "260137ffb9544dca84543e1a7140a983",
            "d56ec1ab838d4b34bf010b64722f88a3",
            "3be028e1883d44afbe2fd6b17560147e",
            "99a80fba12674bbc8ead54a6e8112243",
            "343b206150a144e1b956f86886a585de",
            "7f2c4e82e2b6404099ceab5c1b51f884",
            "a5fec214606f4ebf89271f145410addf",
            "9050e8d5fa564b609b33e6cdb8866e9d",
            "c1f229c767744f8ea62d13918cdc144e",
            "d000f251a61d435b81a8fd6e5f7c74e0",
            "aa7fa12962404d859aaab9f1a3b322d0",
            "b4136b0d4d19457c8bab81c6bb19f3cc",
            "29e82b9d3bb146aa973199ebef3061dd",
            "c65577cd8fa64ca0a848ede78ab230b0",
            "5183b3130d8c443aa91dac212c9840b7",
            "4896f1e6140340268bd8dbb5e090345d",
            "79b7c46d7640411998d25fcd849930dc",
            "ac3a40d698674fbfb459fbf0cfd80932",
            "638a056fe8e140b68c006bd25800ecaf",
            "48bac1d81bcf4e56b2d49497b12e2f78",
            "170b5f6054cb40eb8d17a4852709149f",
            "82bb2aaa2dc14ff692c6ba89cfd41bbb",
            "651ef583dd704cf3b8ad65643e8d8bf3",
            "07ffeafd64174bcfb4c9e2741bd8f455",
            "c334c60e012540f0acde256473bf0d6f",
            "43fc772f91d142cfa082af87231890f1",
            "c4abeb7f21fe4f5e9b4e2b495deae46e",
            "2530a2ea0c5840a091cb9dc6950119ea",
            "1dfd93b82f204129b8c50e487734c29c",
            "699e48af32304d7eafba2b59a58e8023",
            "c7ff257844f7415e8c159be8e9826232",
            "041b7ab3e31f4c0a8af005fcd8534583",
            "3e83baac3a4341c1b494b0d1855f2e91",
            "810b65589af047f6bb6a59d776e0c1a4",
            "6ee4fd55aebd4ca6978d6a91d0560150",
            "98e31ca0e9124b7dbc758bf4cdaa6815",
            "2fb855e406c840f39f2ec5a4689509eb",
            "4166abdd4a974f7f990f953b35329251",
            "af077b37d82e445faeb6415d7da98a75",
            "95b19a012ef249ebb7f8fc584eb1bd05",
            "094dd4153aaa49aaba6f9730d9c861bc",
            "9ba4635f5a9d47a892dfcbc91e43cd25",
            "aec6c48ab9104331abeacdb735abe923",
            "8e5beb3071f44b33866b140d2e26c362",
            "22c3353318f74e64b89b11020fb4fec4",
            "6f6cb19daeb6403db5e1b93fcedf9037",
            "6d936f81671e41f4be8e3c4e627a2ce4",
            "b6c2d39efa8b49a1b252e857c3ab4efb",
            "83475b75766a4251850d94110b4eefb7"
          ]
        },
        "id": "GnHpNle6Dou1",
        "outputId": "cfbf9d15-659d-490b-a9e6-019d5dd87e35"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-a153ccf152c6>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "<ipython-input-57-a153ccf152c6>:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ae9b5dd07f04243938a057bb5ca02a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aced000579ac473988356c0228a85951"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba56dcd023a146938db1f2986f24cc70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaffe9f50e3d4bc590270cd12dd0f5b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe32c2e8112d4d7d9796fd58b0934c89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b29b5a9b5e204b0a8538e85299adec49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c81ae6b3374842a7acd2f049901f77df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f2c4e82e2b6404099ceab5c1b51f884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79b7c46d7640411998d25fcd849930dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2530a2ea0c5840a091cb9dc6950119ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af077b37d82e445faeb6415d7da98a75"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.redis import Redis"
      ],
      "metadata": {
        "id": "x5qTlARpzlAs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rds = Redis.from_documents(\n",
        "    docs_split,\n",
        "    embeddings,\n",
        "    redis_url=REDIS_URL,\n",
        "    index_name=\"youtube\",\n",
        ")"
      ],
      "metadata": {
        "id": "hFrAYl_jDozt"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rds.index_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mYvYqn8cDo4o",
        "outputId": "0f7cd814-4dc2-4376-b4c1-19dae89e5dfd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'youtube'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "id": "3HoxzPQrzs6D"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"data analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnZ0mvWkzws1",
        "outputId": "4c7f4fba-749b-4ab8-c13c-ea89803ce80f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'id': 'doc:youtube:70fc708f921642d087aff0c7af49201e', 'source': 'AOEGOhkGtjI'}, page_content=\"this in a customer data platform now let's try a different one I'll do a classical one show me the\"),\n",
              " Document(metadata={'id': 'doc:youtube:35c545123b7643ce92da0839dfd4e61e', 'source': 'AOEGOhkGtjI'}, page_content='a data analyst or data engineer you should really pay attention to this and learn this new'),\n",
              " Document(metadata={'id': 'doc:youtube:6894d51ed5bb42549692b2a6e7d6ab01', 'source': 'AOEGOhkGtjI'}, page_content='we can then have a look at the data frame and this is essentially an audience that you could use'),\n",
              " Document(metadata={'id': 'doc:youtube:af7bd19c5f3c4bbe988c1a9797d0639d', 'source': 'AOEGOhkGtjI'}, page_content='discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama'),\n",
              " Document(metadata={'id': 'doc:youtube:13c5b1ff4e2047edaf941f803437c31e', 'source': 'AOEGOhkGtjI'}, page_content='the last video in the dashboard video it is an e-commerce data set with four tables customers'),\n",
              " Document(metadata={'id': 'doc:youtube:3e51971adebc4f2fa4e0af87a3706f29', 'source': 'AOEGOhkGtjI'}, page_content=\"going to connect to biy and the data set I'm using is the same data set I used in the last video in\"),\n",
              " Document(metadata={'id': 'doc:youtube:1b8c96651ea74d49a87c812d12e3002f', 'source': 'AOEGOhkGtjI'}, page_content='with the purchase frequency and the average order value so this is very useful so we can use llama'),\n",
              " Document(metadata={'id': 'doc:youtube:4579f32279bd4bb6b0b3aa0f0412973c', 'source': 'AOEGOhkGtjI'}, page_content='to give L three is the following give me a list of the best customers including their rank their'),\n",
              " Document(metadata={'id': 'doc:youtube:021e0fb1f6b4453498606e48bb71f4f6', 'source': 'AOEGOhkGtjI'}, page_content='when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to'),\n",
              " Document(metadata={'id': 'doc:youtube:e0bb93ea129f4eddb739849d76a67b30', 'source': 'AOEGOhkGtjI'}, page_content=\"bit crate data set then I'm injecting the main question or the query and finally I'll be injecting\")]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Building a RAG Chain"
      ],
      "metadata": {
        "id": "kjRvkiDLKjIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "dUPzEJYIw_h7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "r6sZGnaMR7tS"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": (lambda x: x[\"question\"]) | retriever,\n",
        "     \"question\": (lambda x: x[\"question\"])}\n",
        "    | prompt\n",
        "    | llm_gpt4\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "hhK2JQjgICGX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=chain.invoke({\"question\":\"What can you do with LLama 3?\"})"
      ],
      "metadata": {
        "id": "gV2K6pb1R-0e"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "EmMcbpIbBpFN",
        "outputId": "bea66805-b749-4e99-8113-d34a354bdb5a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'With LLama 3, you can generate insights, use it for analyzing data, and maximize its performance to extract valuable insights. It is available on platforms like Gro Cloud, which is noted for being a fast and easy way to get started with LLama 3. The model is described as a game-changer in these tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Chain with a Tool"
      ],
      "metadata": {
        "id": "GIj3mBUBKlmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  youtube_search"
      ],
      "metadata": {
        "id": "y0D6VWq9L23L"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import YouTubeSearchTool\n",
        "\n",
        "youtube_tool = YouTubeSearchTool()"
      ],
      "metadata": {
        "id": "zWo5gjMCLeIh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_tool.run(\"Rabbitmetrics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "mGhA0oZ1Ngav",
        "outputId": "a220e740-81f0-45fc-84eb-cc4191df648a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bind the YouTube tool to the LLM\n",
        "llm_with_tools = llm_gpt4.bind_tools([youtube_tool])"
      ],
      "metadata": {
        "id": "gTwJjbupMZma"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg =llm_with_tools.invoke(\"Rabbtimetrics YT videos\")"
      ],
      "metadata": {
        "id": "IP2vlJ0B0uDW"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg"
      ],
      "metadata": {
        "id": "wwrFygSTDBji",
        "outputId": "ae44573d-c68c-4ba6-82af-ce05f6cc8392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_t6MESNMKDZOSnNa5b24X1JYS', 'function': {'arguments': '{\"query\":\"Rabbtimetrics\"}', 'name': 'youtube_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 97, 'total_tokens': 115, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_92f14e8683', 'id': 'chatcmpl-BLulU5mW6nMmpMXLv7NDGfJWjCi4h', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5b667ef9-fa61-4c78-a43f-23f0f99ba22f-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'Rabbtimetrics'}, 'id': 'call_t6MESNMKDZOSnNa5b24X1JYS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 18, 'total_tokens': 115, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfiebl0C23B0",
        "outputId": "ca21e721-6d29-4ea1-c908-376b65f5c6dc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'youtube_search',\n",
              "  'args': {'query': 'Rabbtimetrics'},\n",
              "  'id': 'call_t6MESNMKDZOSnNa5b24X1JYS',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=llm_with_tools | (lambda x: x.tool_calls[0][\"args\"][\"query\"]) | youtube_tool\n",
        "# chain=llm_with_tools | (lambda x: x.tool_calls[0].args[0]) | youtube_tool\n",
        "# chain=llm_with_tools | (lambda x: x.tool_calls[0].args[\"__arg1\"]) | youtube_tool\n",
        "# chain=llm_with_tools | (lambda x: x.tool_calls[0][\"tool_input\"]) | youtube_tool"
      ],
      "metadata": {
        "id": "D_yIhFdN04rF"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Find some Rabbitmetrics videos on langchain\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "nWOPj_Sj05dU",
        "outputId": "44cb1512-729d-45e9-d6af-21d1125e09bc"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW7SBwkJfgkBhyohjO8%3D', 'https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg =llm_with_tools.invoke(\"Rabbtimetrics YT videos\")"
      ],
      "metadata": {
        "id": "t-kXo30UTk_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1pvZysT2Ra",
        "outputId": "37189afa-3616-45f5-9b7b-3e551337b177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'youtube_search',\n",
              "  'args': {'__arg1': 'Rabbtimetrics'},\n",
              "  'id': 'call_sSmXShg98hIveTbAu84QJCig'}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=llm_with_tools | (lambda x: x.tool_calls[0][\"args\"][\"__arg1\"]) | youtube_tool"
      ],
      "metadata": {
        "id": "JbJQ2DSaUCdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Find some Rabbitmetrics videos on langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XWoXqKDiUaAa",
        "outputId": "77982261-1b98-4539-a65c-64aed580476d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D', 'https://www.youtube.com/watch?v=Xi9Ui-9qcPw&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Building an Agent"
      ],
      "metadata": {
        "id": "rpsmuB_gK0n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet langchainhub"
      ],
      "metadata": {
        "id": "uiEjdI__2svv"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
        "# api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "# print(api_key)  # This will print your API key"
      ],
      "metadata": {
        "id": "yjQTd7KFPauv"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent"
      ],
      "metadata": {
        "id": "uCltklw7kYgX"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rbELzaL-6nh",
        "outputId": "c85fa98f-c10a-4b25-e620-201fde7e2193"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='agent_scratchpad')]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools=[youtube_tool]\n",
        "\n",
        "agent = create_tool_calling_agent(llm_gpt4, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "qQ0Gzyrn_ASh"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Find some langchain YT videos\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_rHfeiD_Ana",
        "outputId": "f519db26-39d5-4ec0-d577-09ad5705ef7d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `youtube_search` with `{'query': 'langchain'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.youtube.com/watch?v=RoR4XJw8wIc&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=1bUy-1hGZpI&pp=ygUJbGFuZ2NoYWlu']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some YouTube videos about langchain:\n",
            "\n",
            "1. [Langchain Video 1](https://www.youtube.com/watch?v=RoR4XJw8wIc)\n",
            "2. [Langchain Video 2](https://www.youtube.com/watch?v=1bUy-1hGZpI)\n",
            "\n",
            "Enjoy watching!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Find some langchain YT videos',\n",
              " 'output': 'Here are some YouTube videos about langchain:\\n\\n1. [Langchain Video 1](https://www.youtube.com/watch?v=RoR4XJw8wIc)\\n2. [Langchain Video 2](https://www.youtube.com/watch?v=1bUy-1hGZpI)\\n\\nEnjoy watching!'}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def transcribe_video(video_url:str) -> str:\n",
        "    \"Extract transcript from YT video\"\n",
        "    loader = YoutubeLoader.from_youtube_url(\n",
        "    video_url, add_video_info=False\n",
        "    )\n",
        "    docs=loader.load()\n",
        "    return docs"
      ],
      "metadata": {
        "id": "hB70blASlbo6"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify YouTubeSearchTool to accept and ignore 'num_results'\n",
        "class YouTubeSearchToolWithNumResults(YouTubeSearchTool):\n",
        "    def _run(self, query: str, **kwargs):\n",
        "        \"\"\"Run the tool.\"\"\"\n",
        "        # Ignore 'num_results' if present\n",
        "        kwargs.pop(\"num_results\", None)\n",
        "        return super()._run(query, **kwargs)\n",
        "\n",
        "youtube_tool_with_num_results = YouTubeSearchToolWithNumResults()\n",
        "tools = [youtube_tool_with_num_results, transcribe_video]\n",
        "\n",
        "\n",
        "\n",
        "# tools = [youtube_tool, transcribe_video]"
      ],
      "metadata": {
        "id": "obRRy20nmLm9"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent = create_tool_calling_agent(llm_gpt4, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "akG5YMcMmYmG"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What topics does the rabbitmetrics YT channel cover?\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc8VvpW8mfXi",
        "outputId": "23b3eb7b-48c5-4bcf-bbb4-ed91dfb0c876"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `youtube_search` with `{'query': 'rabbitmetrics YT channel'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUYcmFiYml0bWV0cmljcyBZVCBjaGFubmVs', 'https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUYcmFiYml0bWV0cmljcyBZVCBjaGFubmVs']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUYcmFiY2hhbm5lbA%3D%3D'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(metadata={'source': 'aywZrzNaKjs'}, page_content=\"blank chain what is it why should you use it and how does it work let's have a look Lang chain is an open source framework that allows developers working with AI to combine large language models like gbt4 with external sources of computation and data the framework is currently offered as a python or a JavaScript package typescript to be specific in this video we're going to start unpacking the python framework and we're going to see why the popularity of the framework is exploding right now especially after the introduction of gpt4 in March 2023 to understand what need Lang chain fills let's have a look at a practical example so by now we all know that chat typically or tpt4 has an impressive general knowledge we can ask it about almost anything and we'll get a pretty good answer suppose you want to know something specifically from your own data your own document it could be a book a PDF file a database with proprietary information link chain allows you to connect a large language model like dbt4 to your own sources of data and we're not talking about pasting a snippet of a text document into the chativity prompt we're talking about referencing an entire database filled with your own data and not only that once you get the information you need you can have Lang chain help you take the action you want to take for instance send an email with some specific information and the way you do that is by taking the document you want your language model to reference and then you slice it up into smaller chunks and you store those chunks in a Victor database the chunks are stored as embeddings meaning they are vector representations of the text this allows you to build language model applications that follow a general pipeline a user asks an initial question this question is then sent to the language model and a vector representation of that question is used to do a similarity search in the vector database this allows us to fetch the relevant chunks of information from the vector database and feed that to the language model as well now the language model has both the initial question and the relevant information from the vector database and is therefore capable of providing an answer or take an action a link chain helps build applications that follow a pipeline like this and these applications are both data aware we can reference our own data in a vector store and they are authentic they can take actions and not only provide answers to questions and these two capabilities open up for an infinite number of practical use cases anything involving personal assistance will be huge you can have a large language model book flights transfer money pay taxes now imagine the implications for studying and learning new things you can have a large language model reference an entire syllabus and help you learn the material as fast as possible coding data analysis data science is all going to be affected by this one of the applications that I'm most excited about is the ability to connect large language models to existing company data such as customer data marketing data and so on I think we're going to see an exponential progress in data analytics and data science our ability to connect the large language models to Advanced apis such as metas API or Google's API is really gonna gonna make things take off so the main value proposition of Lang chain can be divided into three main Concepts we have the llm wrappers that allows us to connect to large language models like gbt4 or the ones from hugging face prompt templates allows us to avoid having to hard code text which is the input to the llms then we have indexes that allows us to extract relevant information for the llms the chains allows us to combine multiple components together to solve a specific task and build an entire llm application and finally we have the agents that allow the llm to interact with external apis there's a lot to unpack in Lang chain and new stuff is being added every day but on a high level this is what the framework looks like we have models or wrappers around models we have problems we have chains we have the embeddings and Vector stores which are the indexes and then we have the agents so what I'm going to do now is I'm going to start unpacking each of these elements by writing code and in this video I'm going to keep it high level just to get an overview of the framework and a feel for the different elements first thing we're going to do is we're going to pip install three libraries we're going to need python.in to manage the environment file with the passwords we're going to install link chain and we're going to install the Pinecone client Pinecone is going to be the vector store we're going to be using in this video in the environment file we need the open AI API key we need the pine cone environment and we need the pine cone API key foreign once you have signed up for a Pinecone account it's free the API keys and the environment name is easy to find same thing is true for openai just go to platform.orgmaili.com account slash API keys let's get started so when you have the keys in an environment file all you have to do is use node.n and find that in to get the keys and now we're ready to go so we're going to start off with the llms or the wrappers around the llms then I'm going to import the open AI Rubber and I'm going to instantiate the text DaVinci 003 completion model and ask it to explain what a large language model is and this is very similar to when you call the open AI API directly next we're going to move over to the chat model so gbt 3.5 and gbt4 are chat models and in order to interact with the chat model through link chain we're going to import a schema consisting of three parts an AI message a human message and a system message and then we're going to import chat open AI the system message is what you use to configure the system when you use a model and the human message is the user message thank you to use the chat model you combine the system message and the human message in a list and then you use that as an input to the chat model here I'm using GPT 3.5 turbo you could have used gpt4 I'm not using that because the open AI service is a little bit Limited at the moment so this works no problem let's move to the next concept which is prompt templates so prompts are what we are going to send to our language model but most of the time these problems are not going to be static they're going to be dynamic they're going to be used in an application and to do that link chain has something called prompt templates and what that allows us to do is to take a piece of text and inject a user input into that text and we can then format The Prompt with the user input and feed that to the language model so this is the most basic example but it allows us to dynamically change the prompt with the user input the third concept we want to Overlook at is the concept of a chain a chain takes a language model and a prompt template and combines them into an interface that takes an input from the user and outputs an answer from the language model sort of like a composite function where the inner function is the prompt template and the outer function is the language model we can also build sequential chains where we have one chain returning an output and then a second chain taking the output from the first chain as an input so here we have the first chain that takes a machine learning concept and gives us a brief explanation of that concept the second chain then takes the description of the first concept and explains it to me like I'm five years old then we simply combine the two chains the first chain called chain and then the second chain called chain two into an overall chain and run that chain and we see that the overall chain returns both the first description of the concept and the explain it to me like I'm 5 explanation of the concept all right let's move on to embeddings and Vector stores but before we do that let me just change the explainer to me like I'm five prompt so that we get a few more words I'm gonna go with 500 Words all right so this is a slightly longer explanation for a five-year-old now what I'm going to do is I'm going to check this text and I'm going to split it into chunks because we want to store it in a vector store in Pinecone and Lang chain has a text bitter tool for that so I'm going to import recursive character text splitter and then I'm going to spit the text into chunks like we talked about in the beginning of the video we can extract the plain text of the individual elements of the list with page content and what we want to do now is we want to turn this into an embedding which is just a vector representation of this text and we can use open ai's embedding model Ada with all my eyes model we can call embed query on the raw text that we just extracted from the chunks of the document and then we get the vector representation of that text or the embedding now we're going to check the chunks of the explanation document and we're going to store the vector representations in pine cone so we'll import the pine cone python client and we'll import pine cone from Lang chain Vector stores and we initiate the pine cone client with the key and the environment that we have in the environment file then we take the variable texts which consists of all the chunks of data we want to store we take the embeddings model and we take an index name and we load those chunks on the embeddings to Pine Cone and once we have the vector stored in Pinecone we can ask questions about the data stored what is magical about an auto encoder and then we can do a similarity search in Pinecone to get the answer or to extract all the relevant chunks if we head over to Pine Cone we can see that the index is here we can click on it and inspect it check the index info we have a total of 13 vectors in the vector store all right so the last thing we're going to do is we're going to have a brief look at the concept of an agent now if you head over to open AI chat GPT plugins page you can see that they're showcasing a python code interpreter now we can actually do something similar in langtune so here I'm importing the create python agent as well as the python Rebel tool and the python webble from nankchain then we instantiate a python agent executor using an open AI language model and this allows us to having the language model run python code so here I want to find the roots of a quadratic function and we see that the agent executor is using numpy roots to find the roots of this quadratic function alright so this video was meant to give you a brief introduction to the Core Concepts of langchain if you want to follow along for a deep dive into the concepts hit subscribe thanks for watching\")]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUYcmFiYml0bWV0cmljcyBZVCBjaGFubmVs'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've broken down the framework into key components and here's an overview of the image I'm going to try to put in your mind in the next 40 minutes there is a lot to unpack but I'll walk you through the components one by one and I've included coding examples with every step I'll start off by asking why length chain and I'll give you a reason why you want to learn this and an overview of the framework then I'll show you how to build chains using prompts and ERS and we'll dive into the runable protocol and Lang chain expression language then we'll look at Splitters and retrievers and we'll build a rag chain finally I'll walk you through how to use tools and how to build an agent with access to Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation for advanced use cases so I'll save Lang graph Lang Smith and Lang surf for separate videos if you're new to Lang chain this tutorial will serve as a quick start and if you're already already familiar with the framework you can use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain to the code and to a written tutorial below this video let's get started I'm going to start off by giving you an overview of the linkchain framework what is it why would you want to use it and then I'm going to show you how to get started developing in a collab notebook so what is Lang chain Lang chain is an open open source framework for developing llm applications an engine allows developers to combine llms with external sources of computation and data and the framework further simplifies the deployment of the llm applications by providing the needed tools such as tools for inspection monitoring and evaluation the deployment and monitoring tools are part of Lang Smith and Lang serf I'm not going to cover that here I'm going to cover the core Library so Lang chain combines llms with external sources of computation and data how does that work suppose that a user wants to ask a question related to some documents or some data that the user has access to now this question will be sent to the llm just like when you're interacting with chat DBT but you also need to fetch the context needed to answer the question from your own data or your own documents and to do that a vector representation of the initial question from the user is used to fetch all the relevant context needed to answer the question typically from a vector database this context is then fed to the llm alongside with the initial question and now the llm has everything it needs to answer the question in the right context and the magical thing about this is that we can do more than just answer the question we can actually take actions based on the input from the end user and we can do that because Lang chain allows us to build agents out of llms and agents are created by by combining llms with tools this will give the llms access to external sources of computation and data so what can we build with Lang chain when it comes to the types of applications you can build with llms this guise the limit from what I see most llm applications that have been built over the last year fall into one or three categories chatbots racun applications or agent systems so the chat BS are nothing new they're used for marketing customer support different types of education and creating assistance what is new is that these chat Bots can now be made more intelligent using llms then we have the rag Q&A applications and this is the magic that Lang chain and llama index made people aware of a year ago so these are the applications that are used for summarizing large documents doing data analysis and generating code by referencing repositories and then we have what we can call agent system both multi-agent systems and systems where we Loop humans into complex interactions with agents and this is what Lang graph is used for and Lang graph is a rather new component in the Lang chain ecosystem the number of potential applications of agent systems is virtually Limitless they're being implemented for Supply Chain management Revenue optimization operations management and so on all right so let's zoom out a bit and have a look at why you want to learn langing so so there are at least two good reasons for learning Lang chain so first of all Lang chain facilitates the transitioning to the next generation of programming languages and what do I mean by that so if we have a look at programming language hierarchies this is what it typically looks like so at the bottom of the pyramid you will have machine language on top of that you will have Assembly Language then you have the higher level languages those are the languages that you are used to program coming in so python Java SQL R C++ and so on and we've been moving up the pyramid ever since programming became a thing and right now we are at the final step the highest level of all languages which is natural language because with the llms we don't have to write code anymore we can have the llms write code for us by giving natural language as an input and the development of these pipelines that allows us to turn natural language into programs is exactly what Lang chain facilitates so we can create pipelines or chains as they are called that allows us to input natural language and get something of value in return like a report an analysis or computer program and with the current generation of best-in-class llms we can already write executable code code that works so this is not something that belongs to the Future it's already here now the second reason you you might want to use Lang chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang graph now Lang chain was late to the multi-agent game you had autogen you had crew AI before that but what langra can do is really remarkable for instance Lang graph allows us to build complex interactions between agents and humans and this is needed especially in big organizations where you have to guard the access to certain files and so on but Lang graph also allows us to build multi-agent systems like autogen or crew II and the use cases for this are becoming more clear every day we need to be able to set up arbitrary communication patterns between agents we also need to do divide and conquer just like humans we can't have an llm do everything so we want to build specialized agents and we want them to be able to perform tasks in an asynchronous way all right let's get started developing the first thing you want to do is you want to fetch the API key from the llm service you want to use so in this tutorial I'm going to use openi and anthropic but Lang chain supports a wide array of llm providers so you can just choose your favorite one once you have grabbed your API key you want to put it in a EnV file all right so here on the collab notebook I have already installed the needed libraries here I'm going to install python. EnV to fetch API keys from the EnV file I'm going to install Lang chain of course Lang chain Community Omi and anthropic and I have uploaded the EnV file from my local machine to the cab notebook and now I'm ready to load the environment variables so I'm loading the API keys and once I've done that I can use chat anthropic and chat Omi to connect to anthropic and Omi and then I'm going to make a test call to GPT 4 and ask what is Lang chain and I simply make an LM call by using the invoke method and here we have an answer from gbt 4 this is very similar to what you would do if you were just calling gbt 4 directly through the open API and of course we can just switch out the llm now I'm calling clot 3 and here I also get a response so I can also easily prpt the llm with a system message and a user message at the same time and I'll just send the two messages in a list to the llm and this will allow you to do some basic prompt engineering but we can do a lot more advanced stuff than this once we start utilizing prom templates and building chains all right so let's dive into chains prompts and loaders and build our first chain I'm going to walk you through what a chain is and give you an analogy to think of then I'm going to work you through prom templates and how to use document loaders so a chain in Lang chain is a sequence of interconnected components that process a user's query utilizing one or more llms to generate and deliver some valuable output and I'm going to get into what those components are in a bit but let me just give you examples of the valuable output that we can generate with the chain so examples of valuable output could be an answer to a specific question it could be a summary of some document it could be a recommendation insights from data a piece of executable code or some Json for an API call and with the chains we can automate the generation of this value now what about the components the components of a chain are typically proms llms or what we call chat models output passes that passes the output of the llm tools that allow the llms to extract additional information from apis or run some code this is what turns llms into agents and general functions and in Lang chain these components are so-called runnables that can be chained together to produce a pipeline and this is one way to think of chains as pipelines in the world of llms so an analogy would be a data pipeline in a data pipeline we typically input some raw data and we get some nice clean data out in a table format and the data cleaning and transformation happening in the pipeline is valuable similarly in an llm chain we input a query and we get some valuable output in return and here the query is turned into value using llm calls functions and additional data and just like with data pipelines we can automate this process with cicd continuous integration and continuous delivery and this is what Lang chain as a framework can help you do so not only can you build the chains but you can also test the chains deploy them monitor the chains and evaluate the output and this can be done with Lang Smith and Lang surf so Lang chain has several built-in modules or classes that helps you create chains efficiently and one of those modules are prompt templates so a prompt for an llm is a set of instructions provided by a user to guide the model's response and promt templates in Lang chain are predefined prompt recipes these recipes are very helpful when you want to create chains fast another feature of Lang chain is that you have so-called document loaders a document is a piece of text and Associated metadata and the Lang chain document loaders provide a load method for loading data as documents from a source into a chain now let's head over to a collab notebook and set up a simple chain I'm going to import prompt template from Lang chain prompts and then I'm going to formulate the prompt to the llm as a string with a placeholder in this case with the variable topic and then I set up the prompt using the prompt template I just imported from Lang chain an input variable that I'm going to call topic and then the string the template itself and this is a simple example of a prompt that now allows me to comp OS a chain and I do that using the pipe operator so I can take the prompt and the language model LL mgbt 4 and assemble a chain using those two components and now I can invoke the chain with an input variable in this case topic so this was a very simple example of assembling a chain using the pipe operator let's try to make this a bit more advanced I'm going to create a chain that will transcribe one of my YouTube videos so I'm going to pip install the YouTube transcript API and then I can use one of the loaders that is spilled into L chain the YouTube loader so I'll import YouTube loader from Lang chain Community document loaders and then I can simply pass the URL to one of my videos or any video to the loader and have the loader extract the transcript from the video now if we have a look at what the loader extracts you can see that we get a list of documents if you want the transcript you can simply extract the page content from the documents or you can simply pass the documents to the chain directly so fetching the raw transcript would be as simple as just fetching the single document in this list and then extracting the page content all right so let's set up a chain that will summarize YouTube videos given a transcript so here I'm going to inject the video transcript into the prom template so this is going going to be my input variable and then I just set up the chain like I did before with the pipe operator and I'm going to invoke it with the video transcript and note that I can just pass the documents I don't have to extract the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a number of build-in chains that you can use for stuff like this for instance the create stuff documents chain and this chain will take a list of documents and pass them all in as context in the prom template pretty much what we did before just without using the buil-in chain this was just a very basic introduction to building chains now in order to build more advanced chains we need to understand Lang chains runnable protocol and Lang chain expression language in more detail let's now try to dive deeper into Lang chain expression language and the runnable protocol the Lang chain expression language simplifies building complex chains from basic components and we do that using the pipe operator that allows us to chain different components together feeding the output from one component to the next so a simple example of a chain composed in this way would be a prompt combined with a model and an output passer and these components are so-called runnables think of Lang chain expression language as a declarative way of composing these so-called runnables into chains and I'll get into what a runnable is in a second here we have a step-by-step example of what a chain looks like we have an input to the chain which is typically a dictionary that is sent to a prompt then a prompt value is sent to the or the chat model then the chat model will return a chat message that we want to pass and we typically want to use the passer to extract a string from the chat message and this will be the output of the chain this is one example of a basic chain and L chain allows us to chain multiple chains together like this so we might want to use the string output of the chain and send that to a new chain that will return a different output so what are runnables let's try to understand the runnable protocol this definition is directly from Lang chain a runnable is a unit of work that can be invoked batched streamed transformed and composed and the chains we built with Lang chain and the components of those chains are runnables and apart from Lang chain native components being runnables we can actually also pass in random functions into chain and those functions will then turn into runnables I'll show you that in an example when we get to the code and these chains can be invoked that means we execute the chain we can call batch on them if we have multiple inputs and we can stream the chains there are some core runnable objects in Lang chain that you want to understand in order to fully appreciate the Lang chain expression language these four objects are the most important ones runnable sequence runnable Lambda runnable pass through and runnable parallel so let's start from the top a runnable sequence in Lang chain is a class that chains together multiple runnable components and it ensures that each component processes its input and sequentially passes its output to the next component in general the change you're building in Lang chain will be of type runnable sequence now what about runnable Lambda runnable Lambda is a class that turns a python callable like a function into a runnable component and this allows us to integrate arbitary functions into chains as in the example I just showed you then we have runnable pass through and a runnable pass through in Lang chain is a class that either passes its input through unchanged or adds additional keys to the output in the first case it acts as a placeholder and in the second case it allows us to do flexible Integrations into sequences where we need to modify the input and finally we have runnable parallel and a runnable parallel in L chain is a class that runs multiple runnables concurrently and this allows you to do branching where you have two chains run the same input but return different outputs now let's see how this works by looking at some code examples all right so I'm going to use the same symbol example throughout this section I'm going to have an llm summarize AI Concepts here I have a prompt that tells the llm to summarize an AI concept and then I'm going to inject a context into the prompt the first thing I'm going to do is I'm going to create a simple chain using the pipe operator we already did it once now I'm going to add an additional element the output passer so the chain is composed of three elements the summarized prompt the llm and the output passer and when we invoke the chain we get the text output now remember that chains are of type runnable sequence and we can check the type of the chain that we just created to see if that is the case so I'm just going to print the type of the chain and here we can see that it is indeed of type runnable sequence now let's try to use a runnable Lambda here we have the same summarized chain as above but now with an additional element so I have a function that will calculate the length of the summary and then I'm wrapping this function in a runable Lambda this allows me to extend the chain we had before with an additional element so by adding this runnable Lambda to the chain from before I now have a chain that will return the length of the summary instead of the summary itself and here I can also check the type of the last element of the chain that I just added and it's of type runnable Lambda now we don't actually need to wrap the function in a runnable Lambda we can just add the function to the chain and then langin will make sure that the function is automatically converted to a runnable Lambda if I print the type of chain with function you can see that the last step of this chain is of runnable Lambda even though we didn't explicitly wrap the function in a runnable Lambda and let me just demonstrate that this chain actually works so I'm just going to invoke chain with function with the same context and here we get the summary length let's have a look at runnable pass through and first we'll go through the case where we use it as a placeholder I'm just going to instantiate a pass through and then I'm going to injected into the chain just before the length Lambda function and and as you can see we get the same output as before I can also check the types of the steps and you can see that the last step is still a runnable Lambda and the step before that is a runnable pass through passing data through without changing it like this is typically used with runnable parallel we can also use a runnable pass through for assigning value to the state of the chain to do that you will use the assign method of the runnable pass through and here I'm going to add the length of the summary to the state of the chain if I then add the assigned pass through as an additional component to the chain the output of the chain will now have an additional key with the length of the summary assigning an additional key value like this is particularly useful when you're working with SQL chains in some of my earlier videos on building custom SQL chains I'm actually using the assign method like this if I run this you can see that we now have the summary and the length and if I check the type of the last component of the chain I now get the type runnable assign the last example I'm going to give is the example of a runable parallel and in this case I will simply take the summary that the summarized chain is returning and then I'm going to Branch Out The Chain in two different arms one is going to take the summary and return the summary as it is this in the other one is going to return the length of the summary and this is going to give me something that is similar to what we got in the previous example so here my output contains the summary and the length of the summary and if I check the type of the last step of the chain I get a runnable parallel now let's talk about splits and retrievers I'm going to walk you through what a retriever is how we can split data into chunks and how we can load those chunks into a vctor store and in the cab notebook I'm going to give you a Hands-On example of working with a vector store I'm going to show you how you can use redis to build a vector store backed Retriever and do retrieval from Lang chain but let's start with answering the question what is a retriever a retriever in Lang chain is an interface case that returns documents given a query and this is not to be confused with a vector store so a vector store can be used as the backbone of a retriever but a retriever does not store the documents itself Vector store back retrievers are by far the most common retrievers when we build llm applications and when we build rag applications retrieval augmented generation applications we typically want to chunk up our data or split our data with a text splitter and store that in the vector store and langin has several built-in text Splitters that can be used for various different purposes it has a recursive text splitter that will try to keep the related chunks of text next to each other it has splitters for HTML markdown different types of code characters and tokens and I'll show you in a bit how to use the recursive text splitter when we get to the notebook so when the data has been chunked up into pieces we loaded into a vector store and then we can use that Vector store as the bases for a retriever that can be used in Chains or agents and if you go back and watch some of my earlier videos there are plenty of examples of using reddis and there's also a few examples of using pine cone but there are many different Vector databases to choose from and it's easy to switch up the vector database if you're using Lang chain because the interface is almost identical for for all the databases all right so let's head over to the collab notebook and see how we can work with Vector stores and how to use Splitters and retrievers so in this example I'm going to be using reddis as the vector store so I've pip installed redis and then I'm going to use the YouTube loader that we used before to extract the transcript from one of my videos here the loader will return a list of documents containing one document with the entire transcript of my video and now I can spit the text in the document using the recursive character text spitter and this text splitter is imported from langin text spitters I can then call the split documents method on the lists of documents containing a single document and then it will return a list of documents with chunks of text I could also have split the raw text by using the split text method of the text splitter once I have the chunks of text I want to load to a vector store I can connect to reddish or any other Vector database that Lang chain supports I'm going to need the host port and password in order to connect to redish and then the full reddish URL which is a combination of the three if you want to connect to redus through Lang chain and we want to do that here to use a hosted version of Reddit like I'm doing here just head over to reddit.com and sign up there a free tier and it's really easy to get started if you want to connect to reddis outside of L chain you can use reddish Pi so here I'm going to import reddis and then I'm connecting to reddis directly through redis py using the host port and password and once the connection has been established I can ping it to see that I actually have a connection and then I'm just going to flush the database to make sure it's empty next next up we're going to need some embeddings and I'm going to use hugging face embeddings since they are free so you'll need to pip install sentence Transformers I already did that here and then you can import hugging face embeddings from Lang chain embeddings and instantiate the embeddings it's going to take a few seconds to import this and when you're done you are ready to load the data to reddis I'm going to import Lang Chain's interface to redus because I want to load the vector data with Lang chain but I could also have done this manually using R pi and again I have an entire video covering that so just head over to the YouTube channel it should be easy to find so to load the vector data to radus we'll use the from documents method that's built into Lang chain Rus interface and as arguments we're going to pass in the list of documents with the chunks of text the embeddings the hugging phas embeddings and the reddish URL and then we're going to give it an index name let's call it YouTube and under the hood Lang chain is using the hogging face embeddings to create numeric vectors out of the text chunks and then load that to the vector database and once we have the data in the vector store we can set up the Retriever and again we use Lang chains radius object and we just call the method s Retriever with a search type and some keyword arguments and now we actually set up to do Vector similarity search over the documents in redish and just like we do with chains if we want to call the retriever we invoke it with a quiry and here we get all the chunks of data that are related in some way to data analysis and this list will contain exactly 10 documents as that's the parameter I set in the search keyword arguments so now we have a retriever and from here it's pretty easy to build a rack chain now let's move on to Rack retrieval augmented generation what is rack so rack is a technique for augmenting at l m knowledge with external data and a rag application is built in two steps the first one is indexing the external data that's what we did in the last section and in the second step we retrieve and generate the output with the llm so you already saw how the first step works we take some documents we chunk them up we turn them into numeric vectors and then we Lo them to a Victor database like reddish and the second step we retrieve and generate so the user query is used to fetch the context from the vector store and then that context is sent to the llm along side with the query itself and this gives the llm what it needs to generate a contextual output in a moment I'm going to show you how to build a rack chain and Lang chain using the retriever we just set up in the last section but before I do that I quickly want to make a comment on something I've noticed that a lot of AI practitioners have been thinking about do we really need to worry about this can we abstract away retrieval when we build chat Bots and rag applications if you're familiar with the OM assistance API you'll know that a lot of this is abstracted away when you build assistance that will do retrieval from a file and on the surface it looks like openi just removed a huge headache and made it a lot easier to build rag applications without having to worry about building retrievers but there is nothing simple about serving the right context to an llm based chatbot so if you're trying to build a chatbot that will convert customers for instance you're really facing a search problem and you generally have two metrics that you want to optimize precision and recall recall measures the completeness of the search results so how many of the relevant documents are we actually fetching and stuffing into the context window and precision measures the quality of the batch we are retrieving so the density of the relevant search results and in order to optimize the conversion rates in the chatbot you need both of these metrics and to do that you need control over the retrieval process which you don't have if you abstract away the retriever so just think about how many businesses are currently doing C conversion rate optimization on web pages and as more and more businesses are implementing chatbots and chat-based interfaces a lot of the C work is going to be focused on optimizing retrievers so I wouldn't bet on abstracting away retrievers at least not in the long run now let's build a rag chain using our reddish based retriever for this example I'm going to use a chat promp template and I'm injecting a context and the users's question into the template then I'm going to use a string output passer that allows me to extract the text output from the llm to set up the chain I'm going to use Lang chain expression language and in the first I'm extracting the question from the input data using the Lambda function and then passing the question to the retriever to get the context so the context here is a composition of two elements and then I also need to pass the question itself to the LM and I do that in the second line and then I use the pipe operator to assemble the entire chain consisting of the first dictionary The Prompt the llm and the string output passer and that's it now we have a rag chain we can then invoke the chain to get an answer to the question what can we do with llama 3 and this answer will then be based on the context which is given by the documents fetched by the retriever moving on to tools so what is a tool what is a toolkit and how do we use a tool tools and Lang chain are interfaces that agents chains or llms can use to interact with the world and I think a good way to think about tools is that just like humans need tools to perform tasks llms need tools as well for instance if you want to do some complex calculation that you can't do in your head you would need a tool for that a calculator or a computer program and similarly for the llm to be able to perform a task it will need access to computational resources or additional data in Lang chain you have several built-in tools you have the python Rebel tool you have a Wikipedia Tool YouTube Tool Saia tool and gradi Tool and so on and more and more tools are constantly being developed Lang chain also has built-in tool kits that are essentially collections of tools that are designed to be used to solve a specific task and examples of toolkits includes the air toolkit GitHub toolkit G lab toolkit and so on and just like tools the number of tool kits available for Lang chain is ever increasing so how do we use a tool and there are essentially two direct ways we can use a tool one is that we can directly run the tool with a query and just like loaders this will typically return some data or we can bind the tools to an llm and then we get an llm with access to tools that we can use in Chains and agents now let me show you how to do that by building on our YouTube example here in the notebook I'm going to pip install YouTube search and then I'm going to import YouTube search tool from Lang chain Community tools and the YouTube search tool allows me to search for YouTube videos so I'll in State the tool and then I can run the tool with the query rabid metric and as you can see this is going to return a list of videos which are videos from my YouTube channel now I can take this tool and I can bind it to the llm and then I get an llm with tools and now I can invoke the llm with tools with a query if I do that you can see that I get an AI message with some search arguments I can extract tool calls and that will give me a clear overview of the arguments and these are the arguments that you need to pass to the YouTube Tool now I can use a Lambda function to extract the arguments and then I can compose a chain consisting of my llm with tools the Lambda function and my YouTube Tool and this allows me to invoke the chain with random queres about YouTube videos so if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me the list of YouTube videos so this is not much different than just running the search but there is now some llm based reasoning behind the crew and this is going to be more interesting once we start building agents finally let's talk about agents I'm going to to explain what an agent is and then we're going to build a YouTube summary agent what are agents so agents are programs capable of reasoning and Performing tasks by integrating llms with tools and they do so by leveraging the lm's capability to generate Json or other structured output this would be a very general definition of the concept in Lang chain agents have a very specific meaning in Lang chain an agent is a class that uses an llm to choose a sequence of actions to take the main conceptual difference between an agent and a chain is that in a chain the sequence of actions are hardcoded whereas in an agent we let the LM decide on what actions to take and what tools to use so think of the chain as a fixed sequence of steps whereas the agent might loop back and use the llm to decide on using a new tool and take a new action and this will go on until the llm decides that the task is completed in theory the agents are more flexible and robust than the chains however we also know that LMS can hallucinate so it depends on the use case now let's build an agent out of our YouTube use case after pip installed Lang chain hop I'm going to use that to extract a pre-made prompt and I'm going to import hop from Lang chain I'm going to import agent executor from Lang chain agents and create tool calling agent from Lang chain agents and the pre-made prom I'm going to use is available on the Hub and there you can see the template that's being used and then there's a variable Called Agent scratch pad and the agent scratch Pad is an important variable it is where the agent will save its intermediary steps but this prompt is ready to use along with our YouTube Tool and the llm and these are the only three things we need to create the agent so I'm going to pass the llm the tools the prompt to create tool calling agent and then I'll set up the agent executor and just like we did with the chains I can now invoke the agent executor and I'm going to ask the agent to find some Lang chain YouTube videos and here we see the list that it extracts with the tool and then it ranks the videos and finishes the chain and if we try to click on the first video we see that it gets Lang chain expained in 13 minutes by rabid metric what a brilliant agent now the cool thing about an agent is that we can just give it access to multiple tools and it will figure out on its own how to use the tools I'm now going to create a custom transcribe video tool using the tool decorator and the YouTube loader we used in section two and this tool will take a video URL and return the transcript of the video as a document then I'm going to create a list of tools with two tools the YouTube search tool which just used and the transcribe video tool and of course you need to have both YouTube search and the YouTube transcript API pip installed to do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me a summary of the topics covered on the channel and here we have the final output with the summary of the topics covered all right that's it for now if you want to dive deeper into Lang chain I suggest you go check out some of the more advanced tutorials on the channel and if you enjoyed this tutorial and want to follow along for more content like this give it a like And subscribe thanks for watching\")]\u001b[0m\u001b[32;1m\u001b[1;3mThe Rabbitmetrics YouTube channel covers topics related to LangChain, an open-source framework for building applications that utilize large language models (LLMs). The videos on the channel include:\n",
            "\n",
            "1. **Introduction to LangChain**: Explaining what LangChain is, why it's beneficial, and how it allows developers to interface LLMs like GPT-4 with external computation and data sources. The channel provides a high-level overview of the LangChain framework and its components, such as LLM wrappers, prompt templates, indexes, chains, and agents.\n",
            "\n",
            "2. **Deep Dive into LangChain Components**: Detailing the core concepts of LangChain, including chains, prompt templates, document loaders, embeddings, and vector stores. The channel provides coding examples in Python, demonstrating how to set up environments, interface with LLMs, and build complex chains using LangChain's robust set of tools.\n",
            "\n",
            "3. **Tools and Agents in LangChain**: Discussing how LangChain integrates various tools and agents, enabling LLMs to perform tasks that require computational resources or external data. The channel explains how to set up tools such as the YouTube search tool or Python REPL tool and how to build agents capable of reasoning and performing complex tasks by interacting with different tools.\n",
            "\n",
            "4. **Applications of LangChain**: The channel explores the practical use cases of LangChain, such as chatbots, retrieval-augmented generation (RAG) applications, and agent systems. It emphasizes the framework's potential to innovate fields like data analytics, customer support, and educational tools by leveraging LLMs.\n",
            "\n",
            "Overall, Rabbitmetrics provides a comprehensive set of tutorials focused on leveraging LangChain for advanced LLM applications, suitable for both beginners looking to understand the basics and advanced users aiming to delve deeper into its capabilities.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What topics does the rabbitmetrics YT channel cover?',\n",
              " 'output': \"The Rabbitmetrics YouTube channel covers topics related to LangChain, an open-source framework for building applications that utilize large language models (LLMs). The videos on the channel include:\\n\\n1. **Introduction to LangChain**: Explaining what LangChain is, why it's beneficial, and how it allows developers to interface LLMs like GPT-4 with external computation and data sources. The channel provides a high-level overview of the LangChain framework and its components, such as LLM wrappers, prompt templates, indexes, chains, and agents.\\n\\n2. **Deep Dive into LangChain Components**: Detailing the core concepts of LangChain, including chains, prompt templates, document loaders, embeddings, and vector stores. The channel provides coding examples in Python, demonstrating how to set up environments, interface with LLMs, and build complex chains using LangChain's robust set of tools.\\n\\n3. **Tools and Agents in LangChain**: Discussing how LangChain integrates various tools and agents, enabling LLMs to perform tasks that require computational resources or external data. The channel explains how to set up tools such as the YouTube search tool or Python REPL tool and how to build agents capable of reasoning and performing complex tasks by interacting with different tools.\\n\\n4. **Applications of LangChain**: The channel explores the practical use cases of LangChain, such as chatbots, retrieval-augmented generation (RAG) applications, and agent systems. It emphasizes the framework's potential to innovate fields like data analytics, customer support, and educational tools by leveraging LLMs.\\n\\nOverall, Rabbitmetrics provides a comprehensive set of tutorials focused on leveraging LangChain for advanced LLM applications, suitable for both beginners looking to understand the basics and advanced users aiming to delve deeper into its capabilities.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zHMz8pmmpqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}