{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "084a0038b5ec449297460deec481693c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb1bc3b542e742a787ac8cdd8b794600",
              "IPY_MODEL_a37869d5478f4b2f82125c9785b9b436",
              "IPY_MODEL_032a03aeee124c3e9a9950df4b3ef432"
            ],
            "layout": "IPY_MODEL_91bb089964f94d2c94f2bc7533c8e25b"
          }
        },
        "fb1bc3b542e742a787ac8cdd8b794600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac2173b7cda4fcfa9e99a630aeb29ac",
            "placeholder": "​",
            "style": "IPY_MODEL_2dc45170227d47e08fdcc4b7cf4b20c0",
            "value": "modules.json: 100%"
          }
        },
        "a37869d5478f4b2f82125c9785b9b436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61691b5d18fc44ecacda46b88f874cc4",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9c4e126c8b14f5e9ce4303fcaa54e09",
            "value": 349
          }
        },
        "032a03aeee124c3e9a9950df4b3ef432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dff85f8149674f459b89803fc7153e3e",
            "placeholder": "​",
            "style": "IPY_MODEL_464269e3d6ca4c5d8211cf2084d2d17e",
            "value": " 349/349 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "91bb089964f94d2c94f2bc7533c8e25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac2173b7cda4fcfa9e99a630aeb29ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc45170227d47e08fdcc4b7cf4b20c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61691b5d18fc44ecacda46b88f874cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9c4e126c8b14f5e9ce4303fcaa54e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dff85f8149674f459b89803fc7153e3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "464269e3d6ca4c5d8211cf2084d2d17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd07a7b9f1b1409ebd3f660f0518e758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b74b2095f564ac78f26ead3ec868d4d",
              "IPY_MODEL_ae04ebe295e34373b74faf3ec1bb2cfa",
              "IPY_MODEL_6c3e6d030a1a4993a812e128cd1cb56d"
            ],
            "layout": "IPY_MODEL_43b529058903409d99e81f4a33b491f5"
          }
        },
        "6b74b2095f564ac78f26ead3ec868d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f60727d06645bbb2f623ebd2db114a",
            "placeholder": "​",
            "style": "IPY_MODEL_dbb5c0fd46194d0594d385e8d60cccb3",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "ae04ebe295e34373b74faf3ec1bb2cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87445bf1b2654d06aae01ac0ed454ad9",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f79fc34cf7914438a17fc10e8185bf10",
            "value": 116
          }
        },
        "6c3e6d030a1a4993a812e128cd1cb56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27b01b2b7e704244afdfb95645db9a71",
            "placeholder": "​",
            "style": "IPY_MODEL_ebf599bae0a64054ae1314c4b1469701",
            "value": " 116/116 [00:00&lt;00:00, 7.28kB/s]"
          }
        },
        "43b529058903409d99e81f4a33b491f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f60727d06645bbb2f623ebd2db114a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb5c0fd46194d0594d385e8d60cccb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87445bf1b2654d06aae01ac0ed454ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79fc34cf7914438a17fc10e8185bf10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27b01b2b7e704244afdfb95645db9a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebf599bae0a64054ae1314c4b1469701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bd138a3e01640e997810f9b71264c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06208b7d09b44c4894d61eea32db0b82",
              "IPY_MODEL_ebbd4cbedf664d168116b13629478f4c",
              "IPY_MODEL_19ff658c4aa14f0f8c623bf87552eb47"
            ],
            "layout": "IPY_MODEL_9c184e70490e4f4b8e620774736de029"
          }
        },
        "06208b7d09b44c4894d61eea32db0b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d030715ed44f2cb0aa3c46dcf78156",
            "placeholder": "​",
            "style": "IPY_MODEL_67571c30f4da4e9095c5ebe5dd05ac1f",
            "value": "README.md: 100%"
          }
        },
        "ebbd4cbedf664d168116b13629478f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d97561ad2844f6a44ecdb5099cc1e9",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c8197a5208b46e8a7be9eb622f61c93",
            "value": 10621
          }
        },
        "19ff658c4aa14f0f8c623bf87552eb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27dd7624edc644e48c7a245c90dbe46c",
            "placeholder": "​",
            "style": "IPY_MODEL_67120da8875c492bbfdff8fb995bdb56",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 654kB/s]"
          }
        },
        "9c184e70490e4f4b8e620774736de029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d030715ed44f2cb0aa3c46dcf78156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67571c30f4da4e9095c5ebe5dd05ac1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83d97561ad2844f6a44ecdb5099cc1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8197a5208b46e8a7be9eb622f61c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27dd7624edc644e48c7a245c90dbe46c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67120da8875c492bbfdff8fb995bdb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eddd05e7269c41af98b12f260425e834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_035b4ae8f52e4108b7f9954592d69e2c",
              "IPY_MODEL_fcb9ac37f7dc4ec2992275c805a86b4b",
              "IPY_MODEL_892b052e040b4e318f05812d82691d46"
            ],
            "layout": "IPY_MODEL_0100d6760bc543afa6059631e937af2f"
          }
        },
        "035b4ae8f52e4108b7f9954592d69e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ace348c6e3a4100bf2e113d4c41ee86",
            "placeholder": "​",
            "style": "IPY_MODEL_e368661e1682492b82023b9cc9a98a99",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "fcb9ac37f7dc4ec2992275c805a86b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d370ca4226f436dae3c5e85549ad5f7",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12f6319c71b8490fbf6df0b3bf5ead4d",
            "value": 53
          }
        },
        "892b052e040b4e318f05812d82691d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367d9496c40542818869ca70c201776c",
            "placeholder": "​",
            "style": "IPY_MODEL_a9eeab3b052b466a9f62e9ae6b4b91a6",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.74kB/s]"
          }
        },
        "0100d6760bc543afa6059631e937af2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ace348c6e3a4100bf2e113d4c41ee86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e368661e1682492b82023b9cc9a98a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d370ca4226f436dae3c5e85549ad5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12f6319c71b8490fbf6df0b3bf5ead4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "367d9496c40542818869ca70c201776c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9eeab3b052b466a9f62e9ae6b4b91a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a974d3f931ca48c8bcab79b4d639cb76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76c33f0733aa41cf9a00a417867dd2f2",
              "IPY_MODEL_5b9bb6dddff44a64aed0c4bb9238f551",
              "IPY_MODEL_eb824d8ec7b44726af215fdc8be40813"
            ],
            "layout": "IPY_MODEL_99bc5473f32c48cab946c7a6033b6c2a"
          }
        },
        "76c33f0733aa41cf9a00a417867dd2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_145cf50f6cab4b158d2080ce60059ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_bfaa0777dc214b45843bf9ec431b3748",
            "value": "config.json: 100%"
          }
        },
        "5b9bb6dddff44a64aed0c4bb9238f551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b69bdda9bec24b22a027d77565130704",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_270990277966485eb20a9ae97c1f376d",
            "value": 571
          }
        },
        "eb824d8ec7b44726af215fdc8be40813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44a8073da9cd40a588a02c3df2de2665",
            "placeholder": "​",
            "style": "IPY_MODEL_8a77b322c3e44307821d9fded2e3c25c",
            "value": " 571/571 [00:00&lt;00:00, 28.8kB/s]"
          }
        },
        "99bc5473f32c48cab946c7a6033b6c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "145cf50f6cab4b158d2080ce60059ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfaa0777dc214b45843bf9ec431b3748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b69bdda9bec24b22a027d77565130704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270990277966485eb20a9ae97c1f376d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44a8073da9cd40a588a02c3df2de2665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a77b322c3e44307821d9fded2e3c25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6b3b724429b47869397453137a0faca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb23c3e448494d6c91cc15a0d4b6dbbc",
              "IPY_MODEL_417145bfcaa846d2b5242bce340e714d",
              "IPY_MODEL_a0c615986d944a9cb60836ab3ba3f563"
            ],
            "layout": "IPY_MODEL_73475bbb3fd949ab9ce9cffb4e6157a2"
          }
        },
        "cb23c3e448494d6c91cc15a0d4b6dbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ed01e00cf04185871747df0826cb16",
            "placeholder": "​",
            "style": "IPY_MODEL_5fe0f3049c794d1881a1336dbff1bcd0",
            "value": "model.safetensors: 100%"
          }
        },
        "417145bfcaa846d2b5242bce340e714d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07bcf47ccd984238a55453d229ba9336",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69ff32abfb1d49b58a04519d9e445e02",
            "value": 437971872
          }
        },
        "a0c615986d944a9cb60836ab3ba3f563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_478a39d31ab54a9eb58a7279d174eecb",
            "placeholder": "​",
            "style": "IPY_MODEL_e5d9f0e7a8bf4629958b30d36efeb0cd",
            "value": " 438M/438M [00:03&lt;00:00, 147MB/s]"
          }
        },
        "73475bbb3fd949ab9ce9cffb4e6157a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ed01e00cf04185871747df0826cb16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe0f3049c794d1881a1336dbff1bcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07bcf47ccd984238a55453d229ba9336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69ff32abfb1d49b58a04519d9e445e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "478a39d31ab54a9eb58a7279d174eecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d9f0e7a8bf4629958b30d36efeb0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a30d8a3c02c940c69aaeaf9cc5d46848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_008b7d227ee1443b970bfb176cd5b2b5",
              "IPY_MODEL_8bedbdf27f4a444489a9941ca9c9c4a3",
              "IPY_MODEL_39e9f2d982c54eb39f62927deeda57c4"
            ],
            "layout": "IPY_MODEL_a8b807b24ab6455faf60499936d126d5"
          }
        },
        "008b7d227ee1443b970bfb176cd5b2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a12db68ee6dc472eb02dca08c420f9f3",
            "placeholder": "​",
            "style": "IPY_MODEL_fae25eac95f0409fa1a75ca8be69147c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8bedbdf27f4a444489a9941ca9c9c4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7889bf0482d485c9c2bfd48ce12e666",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ab25e703d944cf6bd57d23dddbee25c",
            "value": 363
          }
        },
        "39e9f2d982c54eb39f62927deeda57c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17725872d9b14c1ca7be6d42f93e2811",
            "placeholder": "​",
            "style": "IPY_MODEL_e1b1425d29c046149b5b7dd43e17b45b",
            "value": " 363/363 [00:00&lt;00:00, 22.9kB/s]"
          }
        },
        "a8b807b24ab6455faf60499936d126d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a12db68ee6dc472eb02dca08c420f9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae25eac95f0409fa1a75ca8be69147c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7889bf0482d485c9c2bfd48ce12e666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab25e703d944cf6bd57d23dddbee25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17725872d9b14c1ca7be6d42f93e2811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1b1425d29c046149b5b7dd43e17b45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e20c8adfc1ea448ea770b476a0b8e1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db8cf6a511c84896940e7e7a03d5a866",
              "IPY_MODEL_79a3336be19b4d6988cf784c89b119d8",
              "IPY_MODEL_fbb977bfbbe149858563e29a346380fe"
            ],
            "layout": "IPY_MODEL_f8e39202a22a4f01bacc6d16efc180e2"
          }
        },
        "db8cf6a511c84896940e7e7a03d5a866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ceadc92017d401496635b5b281c925e",
            "placeholder": "​",
            "style": "IPY_MODEL_879cd0a6d3014bb9805bbdbfe8794487",
            "value": "vocab.txt: 100%"
          }
        },
        "79a3336be19b4d6988cf784c89b119d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4dc1fb6b1dc45139f921563edeea9c7",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f89db308102e40439188ab6500709be5",
            "value": 231536
          }
        },
        "fbb977bfbbe149858563e29a346380fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0084d02f18d4f8d881f8681b7660e3a",
            "placeholder": "​",
            "style": "IPY_MODEL_df213338b072434aa2ced9908d0ba3c0",
            "value": " 232k/232k [00:00&lt;00:00, 1.84MB/s]"
          }
        },
        "f8e39202a22a4f01bacc6d16efc180e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ceadc92017d401496635b5b281c925e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879cd0a6d3014bb9805bbdbfe8794487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4dc1fb6b1dc45139f921563edeea9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f89db308102e40439188ab6500709be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0084d02f18d4f8d881f8681b7660e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df213338b072434aa2ced9908d0ba3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "898e547bb7b9493c99c2ecbec75146bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d1f76e36d834e65bb7425a7205ee7f7",
              "IPY_MODEL_0ded6b95cb3c40559927e77260caa4c3",
              "IPY_MODEL_cc52b15aa02b488085adbb45bed5a43f"
            ],
            "layout": "IPY_MODEL_dcd017f5203640488bffc293dfa061c6"
          }
        },
        "0d1f76e36d834e65bb7425a7205ee7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_029f8e5b75b04fbe9a5ad1658a6557aa",
            "placeholder": "​",
            "style": "IPY_MODEL_7a2257fb33e34d69b960d2636cab3b33",
            "value": "tokenizer.json: 100%"
          }
        },
        "0ded6b95cb3c40559927e77260caa4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_104b82369cd646fe9b04f78ecac6b09c",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fe5a6d57e494914987ebf8ca2640d43",
            "value": 466021
          }
        },
        "cc52b15aa02b488085adbb45bed5a43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d023080eec2547029bf38facff69a6b9",
            "placeholder": "​",
            "style": "IPY_MODEL_c616faf23fa14d598d8fd15cc2866d58",
            "value": " 466k/466k [00:00&lt;00:00, 3.75MB/s]"
          }
        },
        "dcd017f5203640488bffc293dfa061c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "029f8e5b75b04fbe9a5ad1658a6557aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2257fb33e34d69b960d2636cab3b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "104b82369cd646fe9b04f78ecac6b09c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe5a6d57e494914987ebf8ca2640d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d023080eec2547029bf38facff69a6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c616faf23fa14d598d8fd15cc2866d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbe03d7dda4f4912b8db418e14f444f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aeda4b1d6ac246e686813fbee185d39b",
              "IPY_MODEL_5b5c2b418af04548ab93918fb26c1620",
              "IPY_MODEL_5bbcba16869042f795f0e78d3cd5e39e"
            ],
            "layout": "IPY_MODEL_464872045ebc41cdacd5cf89ac591bd6"
          }
        },
        "aeda4b1d6ac246e686813fbee185d39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3acd5c289e2b4839b92a99457c0197a3",
            "placeholder": "​",
            "style": "IPY_MODEL_49b9303aff0a4d26a7897850223221b3",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5b5c2b418af04548ab93918fb26c1620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_550af51366264da6bc279e67ad82f94d",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e246a6a64a774aaa8de93ae6dc7ae96c",
            "value": 239
          }
        },
        "5bbcba16869042f795f0e78d3cd5e39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d521a1a4b35a4a2eba84c37fb376b7ad",
            "placeholder": "​",
            "style": "IPY_MODEL_d2a49bef5cf24039b7a9e62b1642c1b3",
            "value": " 239/239 [00:00&lt;00:00, 9.46kB/s]"
          }
        },
        "464872045ebc41cdacd5cf89ac591bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3acd5c289e2b4839b92a99457c0197a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b9303aff0a4d26a7897850223221b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "550af51366264da6bc279e67ad82f94d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e246a6a64a774aaa8de93ae6dc7ae96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d521a1a4b35a4a2eba84c37fb376b7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a49bef5cf24039b7a9e62b1642c1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef324189b1844b15b711bca7f023007c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d0a797258e24930bf8569ceb0ebeab0",
              "IPY_MODEL_48bb0a3f7c2c4bc285b6763313eda159",
              "IPY_MODEL_6ed5b15e9cb64600a0a33ac01eb2a656"
            ],
            "layout": "IPY_MODEL_7619b5e38a83414b81c27d924571581b"
          }
        },
        "1d0a797258e24930bf8569ceb0ebeab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da6cbd2764fa4fc68ae84426725a8625",
            "placeholder": "​",
            "style": "IPY_MODEL_927e2ba32d7a4573a19d724dbc87b74a",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "48bb0a3f7c2c4bc285b6763313eda159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e4335a16574eaea7084db58ff7ea3f",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac06bdf136ba40da933ba9fbdaeb72d9",
            "value": 190
          }
        },
        "6ed5b15e9cb64600a0a33ac01eb2a656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_286ee328f3ca44a583e4a6e17a7a9a22",
            "placeholder": "​",
            "style": "IPY_MODEL_1fedf56b3bba42a589e37c0452797320",
            "value": " 190/190 [00:00&lt;00:00, 6.88kB/s]"
          }
        },
        "7619b5e38a83414b81c27d924571581b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da6cbd2764fa4fc68ae84426725a8625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927e2ba32d7a4573a19d724dbc87b74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27e4335a16574eaea7084db58ff7ea3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac06bdf136ba40da933ba9fbdaeb72d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "286ee328f3ca44a583e4a6e17a7a9a22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fedf56b3bba42a589e37c0452797320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terrytaylorbonn/365_langchain_rabbit/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing libraries and connect to LLMs"
      ],
      "metadata": {
        "id": "pNH6I4yNTvxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W2OZYI5XlXXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58263be6-9540-464f-c9d3-fefbf6ea5986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/644.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.4/644.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU  \\\n",
        "  python-dotenv \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  openai \\\n",
        "  anthropic \\\n",
        "  langchain-openai \\\n",
        "  langchain-anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the API keys from the .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZbN7ti31Adv",
        "outputId": "7bc4b3e1-9007-4fa2-dd8a-81f25242cfbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to OpenAI and Anthropic\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm_claude3 = ChatAnthropic(model='claude-3-opus-20240229')\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm_gpt4 = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "j8K_zG4D1HqA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that you can use the LLM\n",
        "llm_claude3.invoke(\"What is LangChain?\").content"
      ],
      "metadata": {
        "id": "db48f5Ei1KSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "c40e654c-2628-49af-cecd-9e5954eef5ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain is an open-source framework designed to assist in the development of applications powered by language models. It provides a set of tools and modules that make it easier to build scalable and maintainable applications that leverage the power of large language models (LLMs) like OpenAI's GPT-3.\\n\\nKey features of LangChain include:\\n\\n1. Modular components: LangChain offers a variety of modular components that can be combined and customized to suit specific use cases. These components include prompts, parsers, memory, and agents.\\n\\n2. Support for multiple LLMs: LangChain is compatible with various LLMs, including OpenAI's models, Hugging Face's Transformers, and more. This allows developers to choose the most suitable language model for their application.\\n\\n3. Chain management: LangChain simplifies the process of chaining together multiple components, such as prompts, parsers, and memory, to create complex language model applications.\\n\\n4. Memory management: LangChain provides tools for managing conversation history and context, enabling more coherent and contextually relevant interactions with language models.\\n\\n5. Extensibility: Developers can easily extend LangChain's functionality by creating custom components or integrating existing tools and libraries.\\n\\nLangChain aims to streamline the development process and provide a structured approach to building applications that utilize language models effectively. It is particularly useful for tasks such as chatbots, question-answering systems, text summarization, and more.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that you can use the LLM\n",
        "llm_gpt4.invoke(\"What is LangChain?\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Z6kHSVM28B1F",
        "outputId": "92c40510-6c1f-4c31-bbf2-032060676248"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an open-source framework designed to help developers build applications that leverage large language models (LLMs). It provides a structured approach to integrating LLMs with other data sources and workflows. LangChain is especially useful for creating applications that require complex interactions with text data, such as chatbots, text analysis tools, and question-answering systems.\\n\\nThe framework focuses on key areas such as:\\n\\n1. **Prompt Management**: Facilitating the efficient creation and management of prompts, which are crucial for guiding the behavior of language models.\\n\\n2. **Chaining**: Allowing developers to create sequences or chains of calls to language models and custom logic, enabling more complex interactions and responses.\\n\\n3. **Indexing and Retrieval**: Providing tools to build and use indexes over document collections, enabling enhanced search and retrieval capabilities.\\n\\n4. **APIs and Integrations**: Offering built-in integrations with various APIs and data sources to expand the capabilities of language models beyond what they can achieve independently.\\n\\n5. **Memory Management**: Enabling applications to maintain conversational memory, which is essential for creating more natural and coherent interactions with users over time.\\n\\nOverall, LangChain is utilized by developers looking to simplify the integration of LLMs into their applications, reduce development time, and create sophisticated language-based applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic request using system and human/user message\n",
        "\n",
        "system_prompt=\"\"\"\n",
        "You explain things to people like they are five year olds.\n",
        "\"\"\"\n",
        "user_prompt=f\"\"\"\n",
        "What is LangChain?\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import textwrap\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_prompt),\n",
        "]"
      ],
      "metadata": {
        "id": "sjfYGNmd1OfD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm_gpt4.invoke(messages)\n",
        "answer = textwrap.fill(response.content, width=100)"
      ],
      "metadata": {
        "id": "fIOYvhTR1u2X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "id": "tulS9RsH1r86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f039a9e2-3f73-438e-a8a6-d0a38ec0558b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alright, imagine you have a big box of Lego blocks, and each block can do something cool. Some can\n",
            "make sounds, some can light up, and others can move things around. But what if you want to build\n",
            "something amazing that uses all these different blocks together? That's where LangChain comes in.\n",
            "LangChain is like a super smart guide that helps you put all these Lego blocks together in a special\n",
            "way. Instead of Lego blocks, LangChain helps connect different parts of computer programs that work\n",
            "with language. It's like building a cool robot that can understand and talk to people, using\n",
            "different pieces that each do something unique.  So, LangChain helps people create smart computer\n",
            "programs that can read, write, and understand human language by connecting different pieces, just\n",
            "like building with Lego!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm_claude3.invoke(messages)\n",
        "answer = textwrap.fill(response.content, width=100)"
      ],
      "metadata": {
        "id": "lADKsC7F8mWc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfWW32-z8wse",
        "outputId": "54aef868-79ba-4280-aafa-e431ef8e570c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a tool that helps computers talk to each other better. You know how sometimes when you\n",
            "play with your friends, you have to explain the rules of the game so everyone understands? LangChain\n",
            "does something similar for computers.  It's like a special language that computers use to share\n",
            "information and work together on tasks. With LangChain, different computer programs can easily\n",
            "connect and understand each other, just like how you and your friends can play together nicely when\n",
            "you all know the rules.  This makes it easier for people who make computer programs to build cool\n",
            "things, like games or apps that can do lots of different tasks. LangChain helps the different parts\n",
            "of the program work together smoothly, just like how all the pieces of a puzzle fit together to make\n",
            "a nice picture.  So, in simple words, LangChain is a helpful tool that lets computers communicate\n",
            "and cooperate better, making it easier for people to create awesome programs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chains, Prompts and Loaders"
      ],
      "metadata": {
        "id": "jFugvpEcTpdR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGF_96Sq8kcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "mhgtPPEcN_dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple prompt template\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following input:\n",
        "{topic}\n",
        "Provide an explanation of the given topic.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt from the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "7NZqTnzIOL0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the chain using the pipe operator \"|\", more on that later\n",
        "chain = prompt | llm_gpt4"
      ],
      "metadata": {
        "id": "PDwGnPSeP_q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\":\"What is LangChain\"}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "Wcb1W45pQEFJ",
        "outputId": "50400825-244f-4d39-a819-3f65b3def2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a framework designed to facilitate the development of applications powered by large language models (LLMs). It focuses on enabling the creation of applications that are data-aware, meaning they can connect with various data sources and utilize this information effectively, and agentic, which implies they can interact with their environment dynamically based on user input and context.\\n\\nKey components of LangChain include:\\n\\n1. **Data Connection**: LangChain allows applications to access and retrieve information from different data sources, such as databases, APIs, or other repositories. This capability helps in grounding the responses from language models in real-world data, making them more accurate and relevant.\\n\\n2. **Agentic Capabilities**: Applications built with LangChain can act autonomously to some extent. They can make decisions, interact with users, and perform tasks based on the context and input they receive. This makes the applications more interactive and responsive to user needs.\\n\\n3. **Modular Design**: LangChain is designed to be highly modular, allowing developers to easily integrate various components and customize their applications according to specific requirements. This modularity makes it easier to build complex applications by combining different functionalities.\\n\\n4. **Scalability and Flexibility**: The framework supports scalability, enabling the development of applications that can handle large amounts of data and complex interactions. It is also flexible, allowing integration with various tools and platforms to extend its capabilities.\\n\\n5. **Focus on LLMs**: While LangChain is versatile, its primary focus is on leveraging large language models. These models, like GPT-4, are capable of understanding and generating human-like text, making them suitable for a wide range of applications, from chatbots and virtual assistants to content generation and data analysis.\\n\\nOverall, LangChain is a powerful tool for developers looking to build sophisticated, AI-driven applications that can interact with data sources and operate dynamically in response to user input. By leveraging the capabilities of large language models, LangChain helps create more intelligent and responsive applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  youtube-transcript-api"
      ],
      "metadata": {
        "id": "a6L3kvBFRasB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Youtube Loader from the LangChain community\n",
        "\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
        ")"
      ],
      "metadata": {
        "id": "FTkY3wQAOztq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the video transcript as documents\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "UuclzSCERocY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwFI1F1hneGi",
        "outputId": "2fad1574-6119-40e6-df43-788440d86f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\", metadata={'source': 'AOEGOhkGtjI'})]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript=docs[0].page_content"
      ],
      "metadata": {
        "id": "4RWjlF2RRrFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now use the transcript in a chain\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
        "{video_transcript}\n",
        "Give a summary.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"video_transcript\"],\n",
        "    template=prompt_template,\n",
        ")"
      ],
      "metadata": {
        "id": "kjtcIV1mR1FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_gpt4"
      ],
      "metadata": {
        "id": "jXALdqNiSP3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that we can just feed the chain the docs without extracting the content as text\n",
        "\n",
        "chain.invoke({\"video_transcript\":docs}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "_6RbQR5ESUDR",
        "outputId": "55a664d8-a096-4717-f40f-3dc8e7b28f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the video, the presenter discusses the potential of Meta's new language model, Lama 3, specifically the 70B model, for advanced data analysis using text-to-SQL chains. The model is noted for its speed and capability to generate sophisticated SQL queries, almost on par with high IQ language models when a specific tweak is implemented. The video demonstrates how to set up Lama 3 on Gro Cloud, the fastest and easiest way to get started, and shows the process of configuring the necessary environment, fetching schema information from a dataset, and building SQL chains using LangChain.\\n\\nThe setup involves:\\n1. Installing required libraries and connecting to a data set.\\n2. Using functions to extract schema information.\\n3. Setting up Lama 3 on Gro Cloud and configuring the SQL chain.\\n\\nThe presenter also introduces a method to make the SQL chain self-correcting by catching errors and feeding them back into the chain, which is particularly useful for lower IQ models. Through several examples, the effectiveness of this approach is demonstrated by generating insights and fixing errors on subsequent attempts.\\n\\nThe implications of using Lama 3 for data analysis include:\\n1. The use of open-source LLMs for generating insights, which is beneficial for privacy-sensitive use cases.\\n2. The potential for reducing costs in query-heavy applications.\\n3. The promise of real-time inference for consumer-facing applications.\\n4. The future role of LLMs in generating data pipelines, dashboards, and reports, highlighting the importance for data analysts and engineers to familiarize themselves with this technology.\\n\\nOverall, the video emphasizes the transformative impact of Lama 3 and similar LLMs on data analysis and the importance of adopting these new tools.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "14-68lwFSsxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The create_stuff_documents_chain takes a list of docs and formats them all into a prompt\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following context:\n",
        "{context}\n",
        "Summarize what Llama 3 can do.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "chain = create_stuff_documents_chain(llm_gpt4, prompt)"
      ],
      "metadata": {
        "id": "OYI-Ej1DSw9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs"
      ],
      "metadata": {
        "id": "vKAUFQElTPHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"context\": docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "Lb_RBPa2TSNf",
        "outputId": "c13389ad-809a-4e89-a790-743ab3114000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Llama 3, developed by Meta, is a powerful language model that has significant capabilities for data analysis using large language models (LLMs). Here are the key features and functionalities of Llama 3:\\n\\n1. **Text-to-SQL Generation**: Llama 3 excels in converting natural language queries into SQL queries. It's fast and capable of generating advanced SQL code.\\n\\n2. **Self-Correcting SQL Chains**: By implementing a tweak where error messages from SQL execution are fed back into the model, Llama 3 can self-correct and refine its SQL generation process. This makes it more reliable even when initial attempts fail.\\n\\n3. **Insight Extraction**: The model can extract detailed insights from data, such as generating customer lists, breaking down revenue by acquisition channels, and identifying top customers based on specific criteria.\\n\\n4. **Privacy-Sensitive Use Cases**: Llama 3 can be used in privacy-sensitive scenarios where customer data needs to be handled carefully. It's particularly useful for applications where data privacy is a concern.\\n\\n5. **Cost Efficiency for Query-Heavy Applications**: Given its ability to handle heavy query loads, Llama 3 is suitable for large organizations looking to implement text-to-SQL applications cost-effectively.\\n\\n6. **Real-Time Inference**: When deployed on platforms like Gro Cloud, Llama 3 supports real-time inference, which is beneficial for consumer-facing applications that require quick responses.\\n\\n7. **Future of Data Analysis**: The model indicates a shift towards LLM-generated data pipelines, dashboards, and reports, suggesting that data analysts and engineers need to adapt to this emerging technology.\\n\\nOverall, Llama 3 represents a significant advancement in LLM capabilities for data analysis, offering robust, real-time, and privacy-conscious solutions for generating and executing SQL queries.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. LCEL & Runnables"
      ],
      "metadata": {
        "id": "HDpbfZb1TknL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "summarize_prompt_template = \"\"\"\n",
        "You are a helpful assistant that summarizes AI concepts:\n",
        "{context}\n",
        "Summarize the context\n",
        "\"\"\"\n",
        "\n",
        "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)\n"
      ],
      "metadata": {
        "id": "HhaQozKlrnkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "848txAD_o8kR",
        "outputId": "6ee322b3-d6de-4be2-aa23-f2904168301a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context'], template='\\nYou are a helpful assistant that summarizes AI concepts:\\n{context}\\nSummarize the context\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Chain with the \"|\" operator"
      ],
      "metadata": {
        "id": "r264yXoiv7-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "NPx6Qz_bTiwz",
        "outputId": "b89f15be-61e8-408b-8115-76ae6fef90af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a framework designed to streamline the development of applications that utilize large language models (LLMs). It offers a comprehensive suite of tools and components that assist in creating applications with features such as:\\n\\n1. **Prompt Management**: Tools for constructing and optimizing prompts to interact with LLMs effectively.\\n2. **Chains**: Mechanisms to link multiple calls to LLMs or other services, creating more complex workflows.\\n3. **Data Augmented Generation**: Techniques to incorporate external data into LLM outputs for more accurate and relevant responses.\\n4. **Agents**: Components that enable LLMs to make decisions and take actions, enhancing their interactivity.\\n5. **Memory**: Features that allow applications to have stateful interactions, remembering past exchanges and using that context in future interactions.\\n\\nLangChain supports integration with various external data sources, including APIs and databases, and is compatible with multiple LLM providers. Its modular design makes it easy to customize and extend, catering to specific application needs, such as chatbots, generative question-answering systems, or interactive agents.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the type of the chain\n",
        "print(type(chain)) # Should print <class 'langchain_core.runnables.base.RunnableSequence'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW3dgnfXKU8r",
        "outputId": "e802cd49-00c6-41d7-e00d-cf672dd206ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a RunnableLambda"
      ],
      "metadata": {
        "id": "ke-ycihtrxpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject python functions into a chain with RunnableLambda\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Define a custom lambda function and wrap it in RunnableLambda\n",
        "length_lambda = RunnableLambda(lambda summary: f\"Summary length: {len(summary)} characters\")\n",
        "\n",
        "lambda_chain = summarize_chain | length_lambda\n",
        "\n",
        "lambda_chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "id": "GEs1q_b6Kgs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fc65923-56e4-48ba-bd20-cd6da84f6c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 631 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(lambda_chain.steps[-1])) # Should print <class 'langchain_core.runnables.base.RunnableLambda'>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-OhiwW33iiU",
        "outputId": "f964f77a-1fba-494b-c4c8-0ebc58d7af23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use function in chain without converting to RunnableLambda\n",
        "chain_with_function = summarize_chain |  (lambda summary: f\"Summary length: {len(summary)} characters\")"
      ],
      "metadata": {
        "id": "JI2Y4s7oUYOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(chain_with_function.steps[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZgqYfgj3hS1",
        "outputId": "1236a785-66ac-498e-c673-4bc896ab04ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_function.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q3x7H60speRq",
        "outputId": "c4850466-076c-415e-ca29-eea529f6859b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 1167 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnablePassthrough as placeholder"
      ],
      "metadata": {
        "id": "75J0BqR3uKpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Create a RunnablePassthrough instance\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Create the sequence using the pipe operator with summarization and length calculation\n",
        "placeholder_chain = summarize_chain| passthrough | length_lambda\n",
        "\n",
        "placeholder_chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p6rAREimtqLi",
        "outputId": "d241c57a-eb77-42d7-c67d-b270246bd7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 788 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(placeholder_chain.steps[-1]))  # Should print <class 'langchain_core.runnables.base.RunnableLambda'>\n",
        "print(type(placeholder_chain.steps[-2]))  # Should print <class 'langchain_core.runnables.passthrough.RunnablePassthrough'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eZz4CEpviLa",
        "outputId": "5115c76b-9865-4e18-de30-210368e9d07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
            "<class 'langchain_core.runnables.passthrough.RunnablePassthrough'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnablePassthrough for assignment"
      ],
      "metadata": {
        "id": "Cc8VpIM4uQxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom lambda function to wrap the summary in a dictionary\n",
        "wrap_summary_lambda = RunnableLambda(lambda summary: {\"summary\": summary})\n",
        "\n",
        "# Create a RunnablePassthrough instance that assigns additional information\n",
        "assign_passthrough = RunnablePassthrough.assign(length=lambda x: len(x[\"summary\"]))\n",
        "\n",
        "# Create the summarization chain\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser | wrap_summary_lambda\n",
        "\n",
        "# Create the full chain combining summarization and assign_passthrough\n",
        "assign_chain = summarize_chain | assign_passthrough\n",
        "\n",
        "# Use the chain\n",
        "assign_chain.invoke({\"context\": \"What is LangChain?\"})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITVOr1PJ9u5z",
        "outputId": "2c83f30c-c625-434b-e111-c3c6c2e867a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'LangChain is a framework designed to simplify the creation and management of applications that utilize large language models (LLMs). It provides tools and abstractions for connecting LLMs with various data sources, enabling more complex and useful applications. LangChain is particularly useful for tasks like document retrieval, question answering, and integrating LLMs with other software systems. It addresses common challenges in working with LLMs, such as managing input/output, handling context, and optimizing performance. By offering a structured approach to these issues, LangChain helps developers build more robust and efficient AI-driven applications.',\n",
              " 'length': 663}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(assign_chain.steps[-1])) # Should print <class 'langchain_core.runnables.passthrough.RunnableAssign'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilA-25jYaITV",
        "outputId": "8144ccae-ac62-4b0f-e54c-c728c212b4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.passthrough.RunnableAssign'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using RunnableParallel"
      ],
      "metadata": {
        "id": "XhRfKg_Wws9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# Create the summarization chain\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Create a RunnableParallel instance to handle summary and length in parallel\n",
        "parallel_runnable = RunnableParallel(\n",
        "    summary=lambda x: x,  # Passes the summary as is\n",
        "    length=lambda x: len(x)  # Calculates the length of the summary\n",
        ")\n",
        "\n",
        "# Combine the summarization chain with parallel runnable\n",
        "parallel_chain = summarize_chain | parallel_runnable\n",
        "\n",
        "parallel_chain.invoke({\"context\": \"What is LangChain?\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwgwh2Czzx75",
        "outputId": "76d734a2-bf09-46f5-82e8-d0b46e8bf128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'LangChain is a framework designed to simplify the creation of applications that leverage the power of large language models (LLMs). It provides tools and abstractions to make it easier to build applications that can perform tasks such as natural language understanding, text generation, and more. LangChain aims to streamline the development process by offering pre-built components and a structured approach to integrating language models into various applications, enhancing their capabilities in areas like conversational agents, text analysis, and automated content creation.',\n",
              " 'length': 579}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the type of the last element in the chain\n",
        "print(type(parallel_chain.steps[-1]))  # Should print <class 'langchain_core.runnables.parallel.RunnableParallel'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MSV99HovG1f",
        "outputId": "821b0c78-4b98-48ce-f62c-3030dcb98b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableParallel'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Retrievers & Splitters"
      ],
      "metadata": {
        "id": "2xnloFfrKdVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  redis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtV7hn48HkHG",
        "outputId": "5c1025c1-b2c0-4aa1-89b8-035b09179d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/252.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m245.8/252.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Youtube Loader from the LangChain community\n",
        "\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
        ")\n",
        "\n",
        "# Load the video transcript as documents\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "F-16JiI-w-y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9oNaHe89j9j",
        "outputId": "7737661b-2063-4c95-c1e2-d47f1a197848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\", metadata={'source': 'AOEGOhkGtjI'})]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "-8_5WwpHDoFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "EMfIZb4YDoQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "FkKhVCMcDoXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFpE66iXDods",
        "outputId": "3871a215-c2ac-46a1-ee53-f5f3712dc493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='can read about Lama 3 and some of the capabilities of the model you can see how the model compares', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"the model compares to other popular llms specifically the 70b model that I'm going to be using in\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"chains with L chain all right the first thing I'm going to do is I'm going to set up the cop\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"to set up the cop notebook and pip install the required libraries then I'll connect to Bay and\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"connect to Bay and fetch the schema information from tables in a data set I'll be installing\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='have two functions that extracts the schema information from bit query in a schema. py file and', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"schema. py file and this is the same functions that I've been using in the earlier videos that lets\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='videos that lets me extract and feed the scheme information from bitre to the chain all right so', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"chain all right so I'm just going to load the environment variables and then I'm going to connect\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"going to connect to biy and the data set I'm using is the same data set I used in the last video in\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='the last video in the dashboard video it is an e-commerce data set with four tables customers', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='tables customers orders products and customer taxs and the two functions in the schema. py file', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='the schema. py file allows me to extract the schema information from the data set as you can see', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"model name in this case I'm using Lama 370b and the prom template will be injecting three things\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"three things first I'll inject the schema information I'm extracting from the bit crate data set\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"bit crate data set then I'm injecting the main question or the query and finally I'll be injecting\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"I'll be injecting a a message history and the message history is The Tweak I mentioned in the\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"I'm going to catch the error and feed it back to the chain and this allows me to make the chains\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"to make the chains self-correcting which is useful when we're dealing with a model that is of a low\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='path through to to inject the schema information and to inject the messages of the message history', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='the message history that contains the errors then I use my prompt the language model and a string', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='model and a string output passer and this is what we need to generate the SQL code from a prompt', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"code from a prompt now let's move on to generate some insights with this setup I had difficulties\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"function that lets me extract the SQL code from the response and in this extract SQL function I'm\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"SQL function I'm simply using regex to extract the SQL from whatever the language model is\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"language model is returning to wrap it all up I'm creating a function that takes a prompt and a\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"there's an error I'm going to collect the error and feed it back to the chain and try again and in\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='try again and in this way the chain will be self-corrected ing because the llm will understand the', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"will understand the error message okay so let's try this the first prompt I'm going to give L three\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='to give L three is the following give me a list of the best customers including their rank their', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='their rank their first name last name and email and the products they purchased and this one it got', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='and this one it got in the first attempt so the query executed successfully and we can then have a', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='we can then have a look at the data frame and this is essentially an audience that you could use', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='that you could use for marketing purposes you normally create an audience like this in a customer', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"this in a customer data platform now let's try a different one I'll do a classical one show me the\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='one show me the revenue generated in the last 30 days broken down by acquisition Channel and here', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='feeding back the error message makes the second attempt successful and here we have Revenue broken', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"have Revenue broken down by acquisition source let's do another audience let's say I want the top\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='say I want the top 100 customers with the highest purchase frequency but with an under average aov', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='under average aov and again we see that the first attempt fails and the second attempt is', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"second attempt is successful and here we only got the names let's say that we want to include the\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='want to include the frequency and the aov as well to get the full overview and here we see that the', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='we see that the first attempt fails the second attempt also fails but the third attempt is a', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='third attempt is a success and here we have the full audience data frame with the purchase', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='with the purchase frequency and the average order value so this is very useful so we can use llama', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='so we can use llama 3 for generating insights if we just implement this small tweak of catching the', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='of catching the errors and feeding it back to the chain all right so what are the implications of', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='the implications of this first of all we now know that with Lama 3 open source llms can be used to', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='llms can be used to generate insights and this is very good news for privacy sensitive use cases so', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='use cases so use cases where you want to feed sensitive customer data back to the llm and in real', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='you want to use Lama 3 locally this is also very good news for query heavy applications so text', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"so text tosql applications can be query heavy if they're rolled out in a big organization so\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"big organization so there's a cost consideration that might be worth looking into now Gro cloud is\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='going to be really useful for Consumer facing applications where speed is necessary finally I think', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='so on will be llm generated in the future and not so distant future so if you are a data analyst or', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='a data analyst or data engineer you should really pay attention to this and learn this new', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"and learn this new technology all right that's it for now if you enjoyed this video I suggest you\", metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='video I suggest you check out one of the other videos on generating SQL with llm chains thanks for', metadata={'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='chains thanks for watching', metadata={'source': 'AOEGOhkGtjI'})]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REDIS_URL=\"redis://default:your_redis_password@your_redis_host:your_redis_port\"\n",
        "REDIS_HOST=\"your_redis_host\"\n",
        "REDIS_PASSWORD=\"your_redis_password\"\n",
        "REDIS_PORT=\"your_redis_port\""
      ],
      "metadata": {
        "id": "5yQG7bkSHBGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "r = redis.Redis(\n",
        "  host=REDIS_HOST,\n",
        "  port=REDIS_PORT,\n",
        "  password=REDIS_PASSWORD)"
      ],
      "metadata": {
        "id": "sZoU-4ucDojx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.ping()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U_pMl4WDopo",
        "outputId": "42687bd8-ba86-423a-c297-1a947730f0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.flushdb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ACoTRLuL5CU",
        "outputId": "79eb7b16-423a-4d69-9f88-2ce1d6a5dff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls9EpiPnyNJ4",
        "outputId": "3fa867a9-d390-42bd-cfca-176c46aa8cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603,
          "referenced_widgets": [
            "084a0038b5ec449297460deec481693c",
            "fb1bc3b542e742a787ac8cdd8b794600",
            "a37869d5478f4b2f82125c9785b9b436",
            "032a03aeee124c3e9a9950df4b3ef432",
            "91bb089964f94d2c94f2bc7533c8e25b",
            "aac2173b7cda4fcfa9e99a630aeb29ac",
            "2dc45170227d47e08fdcc4b7cf4b20c0",
            "61691b5d18fc44ecacda46b88f874cc4",
            "d9c4e126c8b14f5e9ce4303fcaa54e09",
            "dff85f8149674f459b89803fc7153e3e",
            "464269e3d6ca4c5d8211cf2084d2d17e",
            "fd07a7b9f1b1409ebd3f660f0518e758",
            "6b74b2095f564ac78f26ead3ec868d4d",
            "ae04ebe295e34373b74faf3ec1bb2cfa",
            "6c3e6d030a1a4993a812e128cd1cb56d",
            "43b529058903409d99e81f4a33b491f5",
            "a8f60727d06645bbb2f623ebd2db114a",
            "dbb5c0fd46194d0594d385e8d60cccb3",
            "87445bf1b2654d06aae01ac0ed454ad9",
            "f79fc34cf7914438a17fc10e8185bf10",
            "27b01b2b7e704244afdfb95645db9a71",
            "ebf599bae0a64054ae1314c4b1469701",
            "6bd138a3e01640e997810f9b71264c47",
            "06208b7d09b44c4894d61eea32db0b82",
            "ebbd4cbedf664d168116b13629478f4c",
            "19ff658c4aa14f0f8c623bf87552eb47",
            "9c184e70490e4f4b8e620774736de029",
            "19d030715ed44f2cb0aa3c46dcf78156",
            "67571c30f4da4e9095c5ebe5dd05ac1f",
            "83d97561ad2844f6a44ecdb5099cc1e9",
            "2c8197a5208b46e8a7be9eb622f61c93",
            "27dd7624edc644e48c7a245c90dbe46c",
            "67120da8875c492bbfdff8fb995bdb56",
            "eddd05e7269c41af98b12f260425e834",
            "035b4ae8f52e4108b7f9954592d69e2c",
            "fcb9ac37f7dc4ec2992275c805a86b4b",
            "892b052e040b4e318f05812d82691d46",
            "0100d6760bc543afa6059631e937af2f",
            "0ace348c6e3a4100bf2e113d4c41ee86",
            "e368661e1682492b82023b9cc9a98a99",
            "8d370ca4226f436dae3c5e85549ad5f7",
            "12f6319c71b8490fbf6df0b3bf5ead4d",
            "367d9496c40542818869ca70c201776c",
            "a9eeab3b052b466a9f62e9ae6b4b91a6",
            "a974d3f931ca48c8bcab79b4d639cb76",
            "76c33f0733aa41cf9a00a417867dd2f2",
            "5b9bb6dddff44a64aed0c4bb9238f551",
            "eb824d8ec7b44726af215fdc8be40813",
            "99bc5473f32c48cab946c7a6033b6c2a",
            "145cf50f6cab4b158d2080ce60059ee0",
            "bfaa0777dc214b45843bf9ec431b3748",
            "b69bdda9bec24b22a027d77565130704",
            "270990277966485eb20a9ae97c1f376d",
            "44a8073da9cd40a588a02c3df2de2665",
            "8a77b322c3e44307821d9fded2e3c25c",
            "e6b3b724429b47869397453137a0faca",
            "cb23c3e448494d6c91cc15a0d4b6dbbc",
            "417145bfcaa846d2b5242bce340e714d",
            "a0c615986d944a9cb60836ab3ba3f563",
            "73475bbb3fd949ab9ce9cffb4e6157a2",
            "f5ed01e00cf04185871747df0826cb16",
            "5fe0f3049c794d1881a1336dbff1bcd0",
            "07bcf47ccd984238a55453d229ba9336",
            "69ff32abfb1d49b58a04519d9e445e02",
            "478a39d31ab54a9eb58a7279d174eecb",
            "e5d9f0e7a8bf4629958b30d36efeb0cd",
            "a30d8a3c02c940c69aaeaf9cc5d46848",
            "008b7d227ee1443b970bfb176cd5b2b5",
            "8bedbdf27f4a444489a9941ca9c9c4a3",
            "39e9f2d982c54eb39f62927deeda57c4",
            "a8b807b24ab6455faf60499936d126d5",
            "a12db68ee6dc472eb02dca08c420f9f3",
            "fae25eac95f0409fa1a75ca8be69147c",
            "e7889bf0482d485c9c2bfd48ce12e666",
            "2ab25e703d944cf6bd57d23dddbee25c",
            "17725872d9b14c1ca7be6d42f93e2811",
            "e1b1425d29c046149b5b7dd43e17b45b",
            "e20c8adfc1ea448ea770b476a0b8e1a4",
            "db8cf6a511c84896940e7e7a03d5a866",
            "79a3336be19b4d6988cf784c89b119d8",
            "fbb977bfbbe149858563e29a346380fe",
            "f8e39202a22a4f01bacc6d16efc180e2",
            "7ceadc92017d401496635b5b281c925e",
            "879cd0a6d3014bb9805bbdbfe8794487",
            "b4dc1fb6b1dc45139f921563edeea9c7",
            "f89db308102e40439188ab6500709be5",
            "d0084d02f18d4f8d881f8681b7660e3a",
            "df213338b072434aa2ced9908d0ba3c0",
            "898e547bb7b9493c99c2ecbec75146bc",
            "0d1f76e36d834e65bb7425a7205ee7f7",
            "0ded6b95cb3c40559927e77260caa4c3",
            "cc52b15aa02b488085adbb45bed5a43f",
            "dcd017f5203640488bffc293dfa061c6",
            "029f8e5b75b04fbe9a5ad1658a6557aa",
            "7a2257fb33e34d69b960d2636cab3b33",
            "104b82369cd646fe9b04f78ecac6b09c",
            "5fe5a6d57e494914987ebf8ca2640d43",
            "d023080eec2547029bf38facff69a6b9",
            "c616faf23fa14d598d8fd15cc2866d58",
            "cbe03d7dda4f4912b8db418e14f444f2",
            "aeda4b1d6ac246e686813fbee185d39b",
            "5b5c2b418af04548ab93918fb26c1620",
            "5bbcba16869042f795f0e78d3cd5e39e",
            "464872045ebc41cdacd5cf89ac591bd6",
            "3acd5c289e2b4839b92a99457c0197a3",
            "49b9303aff0a4d26a7897850223221b3",
            "550af51366264da6bc279e67ad82f94d",
            "e246a6a64a774aaa8de93ae6dc7ae96c",
            "d521a1a4b35a4a2eba84c37fb376b7ad",
            "d2a49bef5cf24039b7a9e62b1642c1b3",
            "ef324189b1844b15b711bca7f023007c",
            "1d0a797258e24930bf8569ceb0ebeab0",
            "48bb0a3f7c2c4bc285b6763313eda159",
            "6ed5b15e9cb64600a0a33ac01eb2a656",
            "7619b5e38a83414b81c27d924571581b",
            "da6cbd2764fa4fc68ae84426725a8625",
            "927e2ba32d7a4573a19d724dbc87b74a",
            "27e4335a16574eaea7084db58ff7ea3f",
            "ac06bdf136ba40da933ba9fbdaeb72d9",
            "286ee328f3ca44a583e4a6e17a7a9a22",
            "1fedf56b3bba42a589e37c0452797320"
          ]
        },
        "id": "GnHpNle6Dou1",
        "outputId": "831e3ca2-e5f5-4eea-cae7-2adb71e6174c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "084a0038b5ec449297460deec481693c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd07a7b9f1b1409ebd3f660f0518e758"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bd138a3e01640e997810f9b71264c47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eddd05e7269c41af98b12f260425e834"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a974d3f931ca48c8bcab79b4d639cb76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6b3b724429b47869397453137a0faca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a30d8a3c02c940c69aaeaf9cc5d46848"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e20c8adfc1ea448ea770b476a0b8e1a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "898e547bb7b9493c99c2ecbec75146bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbe03d7dda4f4912b8db418e14f444f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef324189b1844b15b711bca7f023007c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores.redis import Redis"
      ],
      "metadata": {
        "id": "x5qTlARpzlAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rds = Redis.from_documents(\n",
        "    docs_split,\n",
        "    embeddings,\n",
        "    redis_url=REDIS_URL,\n",
        "    index_name=\"youtube\",\n",
        ")"
      ],
      "metadata": {
        "id": "hFrAYl_jDozt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rds.index_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mYvYqn8cDo4o",
        "outputId": "2df9b38b-e306-453f-a167-825daec88f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'youtube'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "id": "3HoxzPQrzs6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"data analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnZ0mvWkzws1",
        "outputId": "aafab178-9c95-42db-fef4-b4973f93cecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"this in a customer data platform now let's try a different one I'll do a classical one show me the\", metadata={'id': 'doc:youtube:88cdceef33e34f4e8fa968c2a2799268', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='a data analyst or data engineer you should really pay attention to this and learn this new', metadata={'id': 'doc:youtube:5f1ab6946b7944c8b4315ab037cadc29', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='we can then have a look at the data frame and this is essentially an audience that you could use', metadata={'id': 'doc:youtube:89b61c7a1aee408097b69dab48646fcc', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama', metadata={'id': 'doc:youtube:3ea23e317cc94967a803649c70079036', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='the last video in the dashboard video it is an e-commerce data set with four tables customers', metadata={'id': 'doc:youtube:ce80ceb11a1b489c9fb243fdc85060be', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"going to connect to biy and the data set I'm using is the same data set I used in the last video in\", metadata={'id': 'doc:youtube:0d3fab2e29de47dba829baf4086e47b9', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='with the purchase frequency and the average order value so this is very useful so we can use llama', metadata={'id': 'doc:youtube:4a7e51380aa94142b9d44e3114e73ead', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='to give L three is the following give me a list of the best customers including their rank their', metadata={'id': 'doc:youtube:74e4014d3aaa4556911fbabff033e4d4', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content='when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to', metadata={'id': 'doc:youtube:79159f2c370345afa16c7c55621f5fc1', 'source': 'AOEGOhkGtjI'}),\n",
              " Document(page_content=\"bit crate data set then I'm injecting the main question or the query and finally I'll be injecting\", metadata={'id': 'doc:youtube:c72da4716e764a11932300bf7e2b69f3', 'source': 'AOEGOhkGtjI'})]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Building a RAG Chain"
      ],
      "metadata": {
        "id": "kjRvkiDLKjIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "dUPzEJYIw_h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "r6sZGnaMR7tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": (lambda x: x[\"question\"]) | retriever,\n",
        "     \"question\": (lambda x: x[\"question\"])}\n",
        "    | prompt\n",
        "    | llm_gpt4\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "hhK2JQjgICGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=chain.invoke({\"question\":\"What can you do with LLama 3?\"})"
      ],
      "metadata": {
        "id": "gV2K6pb1R-0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "EmMcbpIbBpFN",
        "outputId": "40c73f21-608d-4d54-e45e-30b3fedf5e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'With Llama 3, you can generate insights, analyze data, and compare model capabilities. It is also noted that it can be used on Gro Cloud, which is described as the fastest and easiest way to get started with Llama 3.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Chain with a Tool"
      ],
      "metadata": {
        "id": "GIj3mBUBKlmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  youtube_search"
      ],
      "metadata": {
        "id": "y0D6VWq9L23L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import YouTubeSearchTool\n",
        "\n",
        "youtube_tool = YouTubeSearchTool()"
      ],
      "metadata": {
        "id": "zWo5gjMCLeIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_tool.run(\"Rabbitmetrics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mGhA0oZ1Ngav",
        "outputId": "ff8a9351-9023-4d86-c807-ef4e9cb8c12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=X3ig10tKxPA&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=rOfg-nbkk8A&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bind the YouTube tool to the LLM\n",
        "llm_with_tools = llm_gpt4.bind_tools([youtube_tool])"
      ],
      "metadata": {
        "id": "gTwJjbupMZma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg =llm_with_tools.invoke(\"Rabbtimetrics YT videos\")"
      ],
      "metadata": {
        "id": "IP2vlJ0B0uDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfiebl0C23B0",
        "outputId": "1d97a373-0989-4f30-fff2-abd72cd97066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'youtube_search',\n",
              "  'args': {'__arg1': 'rabbtimetrics'},\n",
              "  'id': 'call_QuPhZfxeAai9QH7IguQJVecA'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=llm_with_tools | (lambda x: x.tool_calls[0][\"args\"][\"__arg1\"]) | youtube_tool"
      ],
      "metadata": {
        "id": "D_yIhFdN04rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Find some Rabbitmetrics videos on langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nWOPj_Sj05dU",
        "outputId": "264e8e4a-4d94-48a9-fbd5-fa08e32561f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D', 'https://www.youtube.com/watch?v=Xi9Ui-9qcPw&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg =llm_with_tools.invoke(\"Rabbtimetrics YT videos\")"
      ],
      "metadata": {
        "id": "t-kXo30UTk_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1pvZysT2Ra",
        "outputId": "37189afa-3616-45f5-9b7b-3e551337b177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'youtube_search',\n",
              "  'args': {'__arg1': 'Rabbtimetrics'},\n",
              "  'id': 'call_sSmXShg98hIveTbAu84QJCig'}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=llm_with_tools | (lambda x: x.tool_calls[0][\"args\"][\"__arg1\"]) | youtube_tool"
      ],
      "metadata": {
        "id": "JbJQ2DSaUCdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Find some Rabbitmetrics videos on langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XWoXqKDiUaAa",
        "outputId": "77982261-1b98-4539-a65c-64aed580476d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D', 'https://www.youtube.com/watch?v=Xi9Ui-9qcPw&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Building an Agent"
      ],
      "metadata": {
        "id": "rpsmuB_gK0n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet langchainhub"
      ],
      "metadata": {
        "id": "uiEjdI__2svv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent"
      ],
      "metadata": {
        "id": "uCltklw7kYgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rbELzaL-6nh",
        "outputId": "0780b3a3-2deb-4fc9-869f-8cf60345d508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
              " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
              " MessagesPlaceholder(variable_name='agent_scratchpad')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools=[youtube_tool]\n",
        "\n",
        "agent = create_tool_calling_agent(llm_gpt4, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "qQ0Gzyrn_ASh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Find some langchain YT videos\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_rHfeiD_Ana",
        "outputId": "b2fc358f-63d8-4d13-d58b-103c213c24ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `youtube_search` with `langchain,5`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=P3MAbZ2eMUI&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=1bUy-1hGZpI&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=mrjq3lFz23s&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=mL_KuQgX9Oc&pp=ygUJbGFuZ2NoYWlu']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some YouTube videos about LangChain:\n",
            "\n",
            "1. [Video 1](https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUJbGFuZ2NoYWlu)\n",
            "2. [Video 2](https://www.youtube.com/watch?v=P3MAbZ2eMUI&pp=ygUJbGFuZ2NoYWlu)\n",
            "3. [Video 3](https://www.youtube.com/watch?v=1bUy-1hGZpI&pp=ygUJbGFuZ2NoYWlu)\n",
            "4. [Video 4](https://www.youtube.com/watch?v=mrjq3lFz23s&pp=ygUJbGFuZ2NoYWlu)\n",
            "5. [Video 5](https://www.youtube.com/watch?v=mL_KuQgX9Oc&pp=ygUJbGFuZ2NoYWlu)\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Find some langchain YT videos',\n",
              " 'output': 'Here are some YouTube videos about LangChain:\\n\\n1. [Video 1](https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUJbGFuZ2NoYWlu)\\n2. [Video 2](https://www.youtube.com/watch?v=P3MAbZ2eMUI&pp=ygUJbGFuZ2NoYWlu)\\n3. [Video 3](https://www.youtube.com/watch?v=1bUy-1hGZpI&pp=ygUJbGFuZ2NoYWlu)\\n4. [Video 4](https://www.youtube.com/watch?v=mrjq3lFz23s&pp=ygUJbGFuZ2NoYWlu)\\n5. [Video 5](https://www.youtube.com/watch?v=mL_KuQgX9Oc&pp=ygUJbGFuZ2NoYWlu)'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def transcribe_video(video_url:str) -> str:\n",
        "    \"Extract transcript from YT video\"\n",
        "    loader = YoutubeLoader.from_youtube_url(\n",
        "    video_url, add_video_info=False\n",
        "    )\n",
        "    docs=loader.load()\n",
        "    return docs"
      ],
      "metadata": {
        "id": "hB70blASlbo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [youtube_tool, transcribe_video]"
      ],
      "metadata": {
        "id": "obRRy20nmLm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent = create_tool_calling_agent(llm_gpt4, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "akG5YMcMmYmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What topics does the rabbitmetrics YT channel cover?\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc8VvpW8mfXi",
        "outputId": "7e213702-62ba-4c0b-ec7e-5b8029b1d4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `youtube_search` with `rabbitmetrics, 5`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.youtube.com/watch?v=rOfg-nbkk8A&pp=ygUNcmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=X3ig10tKxPA&pp=ygUNcmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUNcmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=6sZqtp9f7VM&pp=ygUNcmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=NBseu9f3P5U&pp=ygUNcmFiYml0bWV0cmljcw%3D%3D']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=rOfg-nbkk8A'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content=\"personalization versus segmentation what is the difference why and what do we want to personalize how can this be done with AI machine learning let's have a look you can think of personalization as the Journey of moving from the left to the right on a spectrum we start with a one message to all approach treating the entire audience as a single entity as we move from the left to the right we begin introducing layers of segmentation first by broad criteria like age then by adding interests and further by specific activities as we approach the right the process ends with onetoone communication tailoring content uniquely for individual customers the underlying hypothesis is that by speaking to individual needs and wishes companies can Delight their customers and increase loyalty and revenue a winwin for everyone at the end of the day the goal of personalization is to match any message or experience with the right customer and you want to do this at scale today email segmentation is often manually conducted by categorizing customers based on predefined criteria such as gender age interest and customer lifetime value this data is used to create specific segments each targeted with different content and for every segment marketers will create different versions of content and then run an AB test to identify the winner however this process is heavy as it only allows for testing a few ad hoc segments at a time adjusting and retesting requires manually altering segment definitions and relaunching campaigns and as a consequence fine-tuning campaigns becomes timec consuming limiting the speed of optimization trying to manually match all the different segments with all the different messages or experiences the task becomes insurmountable on the messaging side with each segment potentially needing its own tailored content and considering multiple styles of versions of messages the permutations further multiply while large language models have made mess message generation more feasible the real challenge now shifts to accurately match each of those generated messages to the most suitable segment and to add to this problem the initial segmentation itself may not be optimal the ad hog approach to segmentation might Overlook more nuanced or effective ways to petition the customer base so even if we successfully match messages to segments there's an underlying uncertainty if those segments are the best representation of the audience in the first place this whole process underscores the need for advanced automated tools to optimize both segmentation and message personalization so how do we do that we take the heavy process of matching customers with messaging or experiences out of the customer data platform and that machine learning do the heavy lifting starting from a goal of matching customers with customer experiences we can create different message variations with chat DBT or with the openi API we can then use machine learning to EST at the uplift of various messages by comparing outcomes for each message against the control group once you have the uplift for each message you can rank the messages according to Roi this allows you to automatically use the customer properties to predict or prescribe the most effective message the personalized message recommendations that you get from this can be fed directly into a platform like clavio clavio then makes it easy to automate the delivery of these tailored messages to the customers ensuring they receive the content most likely to elicit a positive response thereby optimizing engagement and potential Revenue the modeling framework needed for this is called potential outcome modeling imagine having a crystal ball that allows you to evaluate consequences of taking different actions instead of forecasting future events based on past patterns as we do with Predictive Analytics we want to assess the potential consequences of different decisions and for that we need causal inference specifically potential outcome model ing evaluating the consequences of potential actions is different than predicting the future based on past data in fact you don't have a Target variable representing what you're trying to optimize as you can always either do a or b what we are aiming for is an estimate of the conditional average treatment effect also called the Kate the Kate refines traditional uplift modeling from AB testing by evaluating the difference in outcomes between variants for a particular subgroup the distribution of potential outcomes of different messaging or different experiences gives an overall view of how different groups might respond enabling a more strategic and personalized targeting the methodology of estimating Kate or the conditional uplift is about understanding the differential impact of an intervention compared to not intervening by modeling the uplift we can determine the most effective messages or incentives or anything really for individual customers this can be employed for any promotional campaign new product products or informational messages generated with generative AI so by segmenting customers based on predicted uplift we can tailor messages or offers to those most likely to respond positively and this ensures that marketing efforts are optimized maximizing revenue and ensuring that customers receive relevant and valuable Communications so if you want to get started doing uplift modeling there are already three good python libraries for this there is uplift ml which is a library by booking.com then you have corsal ml which is a library by Uber also a library specifically created to do uplift modeling and then we have eonl which is a library developed by people at Microsoft that combines econometric with machine learning and is specifically aimed at solving causal inference problems so when it comes to scaling personalization two key tools emerge as game changers we have generative AI that serves as a dynamic tool for Rapid content creation ensuring that messages can be created faster and on the other hand we have csal inference and machine learning that helps us tailor the messages that resonate with the customers on an individual level so by combining the efficiency of generative AI with the pinpoint accuracy of cal machine learning is now become a lot easier for businesses to create a deeply personalized customer experience there are many different components of the customer experience that we can personalize in order to move towards a more fine grained level of targeted communication for instance we can tailor product recommendations headlines ctas or incenses and by utilizing the builtin dynamic template system that most email service providers come with we can personalize in a way that maximizes ouri by doing the heavy lifting with machine learning and AI that's it for now if you enjoyed this video like And subscribe thanks for watching\", metadata={'source': 'rOfg-nbkk8A'})]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=X3ig10tKxPA'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content=\"in this video I'm going to show you how to go from a wish list of business insights to a full working Luca Studio bi dashboard in minutes using large language models the methodology I'm going to show you is platform agnostic you can use this for any SQL database you want and generate beautiful dashboards in record time I'm going to actually build this in a couple of minutes and I'm going to give you the code in a link below this video so you can do this yourself so I'm going to give you an e-commerce example using e-commerce data but it doesn't have to be this could be CRM data or any other data type that you want to use to build your dashboard I'm just going to assume that you have transferred your data to a data warehouse such as Google biery so transferring your data from source to your data warehouse is pretty straight forward you can use a service like Stitch data or you can use a platform like airite to do this it's almost a commodity at this point I've also included a link below this video if you want to do this yourself if you're working on Shopify I have a full tutorial on creating ETL jobs from Shopify to biery and of course the data needs to be stored using a sensible data model but if you're using a service like Stitch data this is going to come out of the box now let's get started building the dashboard so here in Google biery in the Google biery console I have a data set called Ecom data looker where I have four tables I have customers I have orders and I have products and then I have a table called customer tag which contains customer attributes that are fixed over time and this is pretty standard e-commerce data that you can transfer to bit Cree using a service like Stitch data or airb or something similar so what I'm going to do now is I'm going to use the llm framework Lang chain to build views on these four tables and then I'll use these views as a basis for a dashboard and the views are simply SQL queries that are created using a lang chain SQL chain so in the last couple of videos I showed you how to do robust text tosql and L chain and I'll put a link to those videos below this video and these chains work by extracting the schema information from bitc and then using that schema information to formulate a prompt to a high IQ language model such as CLA 3 or gbt 4 and then have that language model return executable SQL and I'm going to use that chain Now to create those views so first I'm going to need a new data set where the views can be stored so I'm going to create a data set called looker Studio views and now I can programmatically feed these views into this new data set and build a dashboard on top of this so heading over to the cup notebook I have my SQL chain here that is built with Lang chain and I have my get schema function that extracts the schema information from bit query and again if you want to see how this works you should check out the robust text tosql video then I have my service account key gbq key. Json and I have a wish list of business insights which is just a text file with prompts essentially and of course I have my EnV file with the API keys in this case I'm using CLA 3 so let me just show how the SQL chain Works before I build the views so I'm asking for the overall repeat purchase rate the generated SQL is sometimes a bit clumsy sometimes it's actually correct so you would have to check this and as you can see the chain returns executable SQL but what I can do now is I can Loop through the list of insights and I can generate SQL for every insight and push that to Pier as a view and then I've written a function that Loops through every inside it generates the SQL based on the inside using the SQL chain and then it tries to execute that SQL if it fails I'm just going to regenerate I'm not going to try to error or anything fancy yet so I'm going to use cloud 3 to generate some names for the views based on the insights and then I'm going to run my main function process insights that takes a data set name the name I just created in bitr the data set for the views it takes my list of insights it takes my view names the SQL chain and the bitc client as input and this function is just going to Loop through the insights and create all the views one by one and you could try to do this in bulk so creating multiple views with one llm call but you'll find that the llm is actually producing a higher quality result if you take these one by one this is going to take maybe 2 minutes to run and then we'll have all the views in bit query and you can see that I'm executing the SQL and I'm transferring the data to a data frame and printing the output and this is for me to take note if I need to change something if there's something that I think is wrong you can see at one of the views which is new Revenue versus repeat Revenue seems to be wrong because we only have zeros in the repeat Revenue column so this SQL statement is wrong I know that I now have to go check that SQL statement or regenerate it so if the pickrate client can't execute the query or if the data looks wrong you know that the query is wrong so there's no point in check in anything until everything executes and until everything looks right then you go back and you check the SQL to make sure that it's actually correct okay so we now have all the views in bit query in the Luca Studio views data set so I'm going to head over to LCA Studio create a new report and then I'm going to import the bit query data source so I'm going to find my data set in my project and here we have all the views that are generated and I have to add these one by one this is a bit annoying and this is going to take maybe a minute so I'm not going to burn your time with this I'm just going to add all these views one by one to the report and once we have the views in here we can start building the actual dashboard but before I built the dashboard I'm just going to go back to the notebook and I'm going to regenerate the new versus repeat Revenue SQL code and this looks correct so I'm going to push this as view I already deleted the other one and to do that I'm just going to use the create Big R view function I wrote for this and now we have an updated view in the data set okay so now we can build the dashboard I'm going to build the first row which is going to consist of kpis or important metrics so we want our Revenue we want new orders repeat order rate average order value and so on these are some of the metrics that you want to monitor for an e-commerce business and I'm simply going to drag these kpis to the top row I have six in total then I'm going to add a new versus repeat Revenue line chart that shows me how much new Revenue did I get over the last 30 days compared to how much much repeat Revenue did I get and to configure the chart you need to select the right data source from the added data sources and then you can simply drag the metrics that you want to visualize to the Chart here we have the new Revenue now we just need to add the repeat Revenue to the same chart I'm also going to add a symbol stacked bar chart showing Revenue broken down by acquisition source and predicted gender there are different ways you can visualize this you just need to select the right data source first and then you need to select the dimensions and the metrics you want to visualize and for this view we can do a breakdown either by gender or by acquisition Channel and the breakdowns you can do depends on what static tags you have in the database here we go this will work then I'm going to add a table showing top product pairs frequently purchased together and again to configure this table I'm just going to add the right data source the view for this and make sure that the dimensions are correct and here we have three dimensions we have product one product two and then then the count of the number of times the products were purchased together the last thing I'm going to add is a Google map showing Revenue by City And I'm going to choose a bubble map for this and again after selecting the right data source this works out of the box the last thing we can do is to style the dashboard by selecting an appropriate theme and then you can make this look nice and of course if you don't like the kpi names at the top you can always shorten them in the queries so changing the SQL Alias that was used but I'm not going to bore you with the styling of this dashboard let's instead quickly have a recap of what I did with the llms so once you have the Lang chain SQL chain including the get schema function that extracts the schema information from the database and you have the function that processes the generated SQL injects it into Bay you have everything you need to build dashboards in record time and of course we haven't even looked at catching Trace backs and feeding errors back to the llm when it makes a mistake if you do that it's going to be even more robust and that's actually needed if you want to build Advanced dashboards if that's something you want me to cover let me know in the comments below now if you want to learn how to build these robust text to SQL chains I suggest you watch the two text to SQL videos and make sure you subscribe for more content like this in the future thanks for watching\", metadata={'source': 'X3ig10tKxPA'})]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=aywZrzNaKjs'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content=\"blank chain what is it why should you use it and how does it work let's have a look Lang chain is an open source framework that allows developers working with AI to combine large language models like gbt4 with external sources of computation and data the framework is currently offered as a python or a JavaScript package typescript to be specific in this video we're going to start unpacking the python framework and we're going to see why the popularity of the framework is exploding right now especially after the introduction of gpt4 in March 2023 to understand what need Lang chain fills let's have a look at a practical example so by now we all know that chat typically or tpt4 has an impressive general knowledge we can ask it about almost anything and we'll get a pretty good answer suppose you want to know something specifically from your own data your own document it could be a book a PDF file a database with proprietary information link chain allows you to connect a large language model like dbt4 to your own sources of data and we're not talking about pasting a snippet of a text document into the chativity prompt we're talking about referencing an entire database filled with your own data and not only that once you get the information you need you can have Lang chain help you take the action you want to take for instance send an email with some specific information and the way you do that is by taking the document you want your language model to reference and then you slice it up into smaller chunks and you store those chunks in a Victor database the chunks are stored as embeddings meaning they are vector representations of the text this allows you to build language model applications that follow a general pipeline a user asks an initial question this question is then sent to the language model and a vector representation of that question is used to do a similarity search in the vector database this allows us to fetch the relevant chunks of information from the vector database and feed that to the language model as well now the language model has both the initial question and the relevant information from the vector database and is therefore capable of providing an answer or take an action a link chain helps build applications that follow a pipeline like this and these applications are both data aware we can reference our own data in a vector store and they are authentic they can take actions and not only provide answers to questions and these two capabilities open up for an infinite number of practical use cases anything involving personal assistance will be huge you can have a large language model book flights transfer money pay taxes now imagine the implications for studying and learning new things you can have a large language model reference an entire syllabus and help you learn the material as fast as possible coding data analysis data science is all going to be affected by this one of the applications that I'm most excited about is the ability to connect large language models to existing company data such as customer data marketing data and so on I think we're going to see an exponential progress in data analytics and data science our ability to connect the large language models to Advanced apis such as metas API or Google's API is really gonna gonna make things take off so the main value proposition of Lang chain can be divided into three main Concepts we have the llm wrappers that allows us to connect to large language models like gbt4 or the ones from hugging face prompt templates allows us to avoid having to hard code text which is the input to the llms then we have indexes that allows us to extract relevant information for the llms the chains allows us to combine multiple components together to solve a specific task and build an entire llm application and finally we have the agents that allow the llm to interact with external apis there's a lot to unpack in Lang chain and new stuff is being added every day but on a high level this is what the framework looks like we have models or wrappers around models we have problems we have chains we have the embeddings and Vector stores which are the indexes and then we have the agents so what I'm going to do now is I'm going to start unpacking each of these elements by writing code and in this video I'm going to keep it high level just to get an overview of the framework and a feel for the different elements first thing we're going to do is we're going to pip install three libraries we're going to need python.in to manage the environment file with the passwords we're going to install link chain and we're going to install the Pinecone client Pinecone is going to be the vector store we're going to be using in this video in the environment file we need the open AI API key we need the pine cone environment and we need the pine cone API key foreign once you have signed up for a Pinecone account it's free the API keys and the environment name is easy to find same thing is true for openai just go to platform.orgmaili.com account slash API keys let's get started so when you have the keys in an environment file all you have to do is use node.n and find that in to get the keys and now we're ready to go so we're going to start off with the llms or the wrappers around the llms then I'm going to import the open AI Rubber and I'm going to instantiate the text DaVinci 003 completion model and ask it to explain what a large language model is and this is very similar to when you call the open AI API directly next we're going to move over to the chat model so gbt 3.5 and gbt4 are chat models and in order to interact with the chat model through link chain we're going to import a schema consisting of three parts an AI message a human message and a system message and then we're going to import chat open AI the system message is what you use to configure the system when you use a model and the human message is the user message thank you to use the chat model you combine the system message and the human message in a list and then you use that as an input to the chat model here I'm using GPT 3.5 turbo you could have used gpt4 I'm not using that because the open AI service is a little bit Limited at the moment so this works no problem let's move to the next concept which is prompt templates so prompts are what we are going to send to our language model but most of the time these problems are not going to be static they're going to be dynamic they're going to be used in an application and to do that link chain has something called prompt templates and what that allows us to do is to take a piece of text and inject a user input into that text and we can then format The Prompt with the user input and feed that to the language model so this is the most basic example but it allows us to dynamically change the prompt with the user input the third concept we want to Overlook at is the concept of a chain a chain takes a language model and a prompt template and combines them into an interface that takes an input from the user and outputs an answer from the language model sort of like a composite function where the inner function is the prompt template and the outer function is the language model we can also build sequential chains where we have one chain returning an output and then a second chain taking the output from the first chain as an input so here we have the first chain that takes a machine learning concept and gives us a brief explanation of that concept the second chain then takes the description of the first concept and explains it to me like I'm five years old then we simply combine the two chains the first chain called chain and then the second chain called chain two into an overall chain and run that chain and we see that the overall chain returns both the first description of the concept and the explain it to me like I'm 5 explanation of the concept all right let's move on to embeddings and Vector stores but before we do that let me just change the explainer to me like I'm five prompt so that we get a few more words I'm gonna go with 500 Words all right so this is a slightly longer explanation for a five-year-old now what I'm going to do is I'm going to check this text and I'm going to split it into chunks because we want to store it in a vector store in Pinecone and Lang chain has a text bitter tool for that so I'm going to import recursive character text splitter and then I'm going to spit the text into chunks like we talked about in the beginning of the video we can extract the plain text of the individual elements of the list with page content and what we want to do now is we want to turn this into an embedding which is just a vector representation of this text and we can use open ai's embedding model Ada with all my eyes model we can call embed query on the raw text that we just extracted from the chunks of the document and then we get the vector representation of that text or the embedding now we're going to check the chunks of the explanation document and we're going to store the vector representations in pine cone so we'll import the pine cone python client and we'll import pine cone from Lang chain Vector stores and we initiate the pine cone client with the key and the environment that we have in the environment file then we take the variable texts which consists of all the chunks of data we want to store we take the embeddings model and we take an index name and we load those chunks on the embeddings to Pine Cone and once we have the vector stored in Pinecone we can ask questions about the data stored what is magical about an auto encoder and then we can do a similarity search in Pinecone to get the answer or to extract all the relevant chunks if we head over to Pine Cone we can see that the index is here we can click on it and inspect it check the index info we have a total of 13 vectors in the vector store all right so the last thing we're going to do is we're going to have a brief look at the concept of an agent now if you head over to open AI chat GPT plugins page you can see that they're showcasing a python code interpreter now we can actually do something similar in langtune so here I'm importing the create python agent as well as the python Rebel tool and the python webble from nankchain then we instantiate a python agent executor using an open AI language model and this allows us to having the language model run python code so here I want to find the roots of a quadratic function and we see that the agent executor is using numpy roots to find the roots of this quadratic function alright so this video was meant to give you a brief introduction to the Core Concepts of langchain if you want to follow along for a deep dive into the concepts hit subscribe thanks for watching\", metadata={'source': 'aywZrzNaKjs'})]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=6sZqtp9f7VM'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content=\"in this video we're going to be building knowledge bases for llm based SQL chains by building a well structured knowledge base consisting of SQL queries organizations can Leverage The expertise of a single SQL expert to empower multiple employees essentially giving employees the skills and tools needed to extract valuable insights from data with llms when using a service like chat DBT to write code you typically need to paste in relevant context for the LM to return something useful the context can be similar code or anything that helps the llm understand the task but this way of working is Impractical and can be automated with a framework like Lang chain that lets you connect llms to needed data sources in a single application knowledge bases also allow us to generate more advanced SQL queries by using F short prompting techniques we can build examples of SQL queries and detailed schema information into llm prompts and this is going to lead to fewer prompting errors and allows to build data products a lot faster with llms here's an overview of how it works you'll have your SQL expert create a collection of relevant SQL queries the queries will be vectorized and loaded into a vector database now you can enable vctor similarity search on these SQL statements essentially building racks for SQL generation you'll need the functions that allows you to extract the detailed schema information from the database this will also be built into the chains now you can do the The Prompt engineering essentially injecting the relevant SQL statements alongside with the schema information into your prompt templates and now you have everything you need to formulate a chain that generates Advanced executable SQL based on a prompt in this video I'm going to be using reddis as the vector database for storing the example queries and then I'll be using Google bit query as the main SQL database we want to query and to illustrate the use of the knowledge base I'm going to attack a well-known problem with fetching analytics data in Big Ray dealing with nested data structures using the UN Nest operator I'll share a link to the tutorial and cab notebook below this video let's get started all right so here we have the collab notebook I'm preparing we need a bunch of libraries we need langin of course we need open AI or anthropic I'm installing both I'm going to use sentence Transformers to create the embeddings escale Alchemy to connect to the SQL database in this case big Ray and we need redis in order to be able to build the knowledge base and of course I have myv file and my gpq key. Json file which is my service account key that allows me to connect to B I'm going to specify a project a data set and the service account key path and then I can connect with SQL database using the full URL and the data set I'm connecting to is the same data set I've used in an earlier video gbq chat which is an e-commerce data set consisting of customers orders and products and it has repeated records and nested feels which makes it a bit more interesting to work with so first I'm going to illustrate the problem we are facing using a simple create SQL chain so one of laying chains off the shelf chains that allows you to chart with the database I'm going to connect to open Ai and anthropic I usually use both gbd4 and clot 3 Opus then I'm going to set up a simple chain using cre SQL chain that connects to the database using gbt 4 and with this chain we can start asking questions and I'm going to ask a very simple question I'm going to ask how many customers are in the data set and as you can see this is leading to a database error and this is a common problem with repeated records now we want the llm to deal with this and I already showed you in the last couple of videos that we can extract detailed schema information from bit cery and build our own chains which is going to make things more robust and now we're going to add an extra element to this which is fuse shot promting that's going to make things even more robust I'm going to use redis as the Victor store to store the SQL query examples that we need for fuse shot promting and to connect to redus I need the host port and password I'm going to connect both with redis pi and with the L chain redis connector the Lang chain Rus connector needs the full Rus URL and I don't have the Rus information in my environment file so I'm setting this with OS then I'm going to import redis which is the reddis P library and I'm going to import reddis from Lang chain Victor stores I'm setting up the connection using reddis piy and the host port and password you don't need the redis pi connection to do this I'm just using it because I'm used to using reddis pie I'm just going to Ping the database make sure we have a connection and we do and then I'm going to flush it and make sure it's empty next up we're going to take a look at the SQL quer samples I have a list of dictionaries here that we're going to use for the fot prompting let me just print the examples so that they are readable so here we have an array of dictionaries consisting of a user input in a b SQL statement and some of those statements contain the unest operator and what we're going to do is we're going to embed these input queries along with the SQL statements so that we can do Vector similarity search over all the different SQL statements and this allows us to fetch relevant SQL statements that we can then inject into the main prompt that we're using to extract insights from the SQL database so to embed these statements I'm first going to create some SQL Vector data which is just joining the input query along with the SQL statement and then I'm going to to import hugging face embeddings you can use any embeddings you want and now that I have my hugging face embeddings I can load the SQL Vector data into redus alongside with my vector embeddings and I'm using L chains redus connector for this and note that I'm not specifying any URL because I already set that with OS and once we've loaded the embeddings and the SQL Vector data we can do a vector similarity search and then I can for instance fetch queries that contain the statement on nest and this will give us a lot of relevant context for the prompt we want to formulate when extracting something from bit query so when you want to do few shot prompting Lang chain has a built-in example selector that will actually do Vector similarity search under the hood and that's called semantic similarity example selector and we can import that from Lang chain core example selectors and then we set it up with the example queries the embeddings and the reddish instance the example selector has a builtin method called select examples which will do the Mi to similarity search and fetch the relevant examples that you need for f short prompting and I'm going to do two different things with this example selector I'm going to use it in a create SQL chain the out of thebox chain from Lang chain and I'm going to use it with the chain we assembled in the last two videos which was a custom chain where we injected our own schema information and I'm going to start with the create SQL chain and to do that I'm importing a standard prom template and a f shot prom template and then I'm injecting table information the number of rows we want to fetch and the input from the user and this is very close to what the Lang documentation suggests I'm going to put a link to the documentation below the video the table information is going to be the schema information that we get from bigr and I'm going to fetch that in the same way as I did in the last two videos using two functions the first of these two functions builds the schema description by iterating all the fields in the tables and the second function uses the first function the data set ID and the bitrate client to fetch the schema information and then I just need a bitr client so I'm going to import bitr from Google cloud and I'm going to import service account from Google or to and then I'm going to use the service account key to set up the connection and this allows me to fetch the detailed schema and table information for the bit crate data set and as you can see here we get the detailed information on the tables and the fields for each table so so now I can inject this into the prom template so I'm passing the gpq schema information to table info so now in our prompt we have the detailed schema information for the tables and we have the SQL statements or the examples of SQL queries and now the create SQL change should have everything it needs to formulate the query based on my input so I'm going to formulate the create SQL chain using GPT 4 the database connection and the prompt we just set up and here we have the same problem once again a problem with array struck and this is why I prefer to take control over the chain building and build my own custom SQL chains and luckily we can easily do that in our case so I'm just going to formulate a prompt alternative and here I just have two input variables I'm injecting the schema so the same schema I was extracting before I'm going to use that again and then I'm injecting the input from the user and I need two things for this I'm going to need a string output passer that's going to pass the output from the LM as a string and I need a runnable pass through then I'm using my fetch schemas function to inject the schema in a runnable pass through and then I'm assembling all the components the runnable pass through the prompt the language model in this case dpt4 and the string output passer to a SQL chain and if I run this and ask how many customers are there I'm going to get a response as a SQL statement and I can just run this statement using my bitc client or you could use SQL Alchemy if you prefer that again like in the two last videos this seems to be the more robust solution at the moment and of course we can also ask more complicated questions such as list the five customers with the highest spent and then we simply execute the response with the bige client so assembling your own chains and injecting your own schema information seems to be a robust solution when you want to generate Advanced SQL and adding F shot prompting to this is going to make it even more robust all right that's it for now if you enjoyed this video like And subscribe thanks for watching\", metadata={'source': '6sZqtp9f7VM'})]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `transcribe_video` with `{'video_url': 'https://www.youtube.com/watch?v=NBseu9f3P5U'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content=\"function calling is one of the coolest features of openi language models by enabling these models to Output Json we can turn them into autonomous agents capable of accessing a range of useful tools but there is more to this feature than just creating agents what if I told you that the traditional tools you're using as a data scientist are about to be transformed in ways you didn't expect in this video I'm going to give you two specific examples from the world of customer analytics first I'll demonstrate how llms could replace traditional machine learning in specific scenarios offering a more effective solution then we'll see how leveraging llm function calling can dramatically accelerate data analysis and insight generation I'm going to be using the open AI API and gbt 4 but keep in mind that what we're doing here can likely be achieved with the right open source model as well it just requires a bit more work by the end of this video you'll have an overview of how you can leverage function calling and the code that lets you work and specific data science use cases more efficiently if you new here like And subscribe I'll put a link to the code below this video so what exactly is function calling in the context of om and llms function calling enables developers to retrieve structured data from omi's gbt Models consistently allowing the gbt to take userdefined functions as input and generate Json output the opening I API will not execute the function but will generate the arguments needed to call the function and the way it works in a not shell is that a user will send a request formulated in natural language the language model is provided with a list of function descriptions that allows the language model to return the Json data needed to make the correct function call this is possible because gbt 4 is specifically fine-tuned to increase function coding accuracy and reliability let's have a look at how this works first by looking at the documentation and then by writing some code all right so here I am in the Omi API reference documentation specifically looking at the chat completion endpoint and as you can see here the most recent version of gb4 allows you to pass in a response format and this allows you to tell the model that you want a Jason object as an output and note that in order for this to be robust and reliable you also need to instruct the model to produce Json either in a system prompt or a user prompt we can also pass in a list of tools right now it's only functions that you can pass to the model and the description name parameters is used by the model to generate Json inputs for those functions so in this video I'm going to focus on the Json part because that is the most fundamental feature we need to understand the implications of so let's head over to a collab notebook and start exploring I'm going to start off with a simple example of an API call that returns Json and then build on that I have my open API key in an environment file and then I'm going to load that using python. EnV then I'm going to import om Ai and instantiate the client and then I'm ready to make a function call I'm not actually going to call a function I'm just going to have the language model return Json and the example we're going to look at is extracting the age and the gender from a customer name this is a classical exercise in customer analytics because it helps marketers Target the messaging to the specific customer there's actually an old article on this in 538 that goes into how this can be done using probabilistic modeling I'll put a link to this article below the video but here we're going to focus on using gbt 4 for this so in this example I have a customer named Karen Backstrom and I'm simply prompting gbt 4 to give me a guess of the age and gender of the customer based on the name I know that I'm specifically asking for adjacent object and then I'm simply passing this prompt to the chat completions endpoint using the gbt 4 1106 preview model and of course I'm specifying that I want the response format to be ajacent object if we look at the content of The Returned message you can see that we get a name because I specified that I wanted a name back we get an estimated age and we get a gender now this is already useful and you can probably see where I'm going with this because we can pass a lot more than the name of the customer to the language model and we can have the language model model tell us what type of buyer Persona Karen is so what is a buyer Persona a buyer Persona is a fictional representation of an ideal customer based on market research and data on existing customers it typically includes details like demographics Behavior patterns motivations and goals any brand should have a clear idea of the target buyer personas based on the product selection alone and if these personas are really representing the customer base then any c customer with a purchase history should somehow fit into one of these fictional representations and as a customer increases engagement with a brand over time more and more data will be available and the best guess of a Persona type will typically change based on the order data and the behavior data the main reason we want to understand the Persona type of a given customer is because it helps the brand communicate with that customer and this is important because this is why language modeling is a useful tool for this so I asked chat dutt about personas for a fashion brand operating in London chat DT has no problem coming up with the different Persona types it'll give you age groups gender interests and behavior and what else you might want and you can ask chat to give you more information if you'd like to refine the groups or you can upload a file with the product catalog which will make the groups even better given these personas I then ask chat dut to place a given customer in one of those categories and here I have have a Karen Backstrom that bought a Fossil women pink dial chronograph watch and as you can see chat can reason about the customer and places Karen in the professional women's category that means we can feed gbt 4 with upto-date information about a customer and have the language model make a best guess about the Persona type now let's write some code for a pipeline that will dynamically update Persona information based on customer data so so I have a personas TT file with 20 personas for a fashion brand similar to what I just showed you in chat DBT now what I want to do I want to use these 20 personas as a basis for making the best guess about the Persona type of a given customer so I'm just going to load the 20 base personas into a python variable and then let's have a look at some specific examples of customer data here I have a development Shopify store with some sample products we have some backpack some shoes and some kits items this Shopify store is linked to the email service provided clavio so that every time an order is made it's going to appear in the clavio dashboard and clavio not only tracks orders but also events on the web page so we can head over to clay.com and log in this is the main dashboard all see and I can fetch profiles of my customers by clicking profiles and then select one of the customers and here I have a John Doe that ordered a Addidas classic backpack and every customer in here has a unique ID that we can use to fetch the customer data that is collected besides John in here I also have Karen Backstrom and she ordered a different backpack the hersel Iona backpack and the unique ID for every customer can be found in the URL or under profile details for every customer in here clavio is collecting events tied to specific metrics so if we click metrics under analytics we can see a list of all the metrics and of course all the metric or the events tied to the metrix contain information that can be used to take the marketing towards a specific customer so I'm going to focus on ordered product and I'm going to fetch all the events tied to this metric for specific customer using the ID of the metric that can be found in the URL as well now let's fetch some data for John and Karen I'm going to use the python rabber around the clavio API you can install that by running pip install clavio API then I'm going to instantiate the API using the clavio API key that I've stored in the environment file and here I have the IDS for John and Karen and then I have a simple python function that allows me to extract all the attributes that are linked to the profiles of the customer as well as the events that are linked to the metric ordered product and the function is just going to return everything as a string so I can feed it to the language model so let me just extract the data for John and as you can see here the data is returned as a string in a raw format there's no feat engineering going on no nothing we're just extracting raw data from the API let me just extract the data for Karen as well and that also works we see that we have the purchase of the heral AO oner back so I'm just going to extract the data for John again and now we want to feed this to the language model to do that I have a Persona prompt that is similar to the one I use to extract the age and gender from the name I will inject the customer data the 20 based personas and then I'll ask the language model to give me the Persona category name the interests the behavior and the age and gender of the customer and let me start by doing this for John I'll call the chat completions endpoint with the Persona prompt asking for Jason in return and here we have the Persona category information for John he's a young professional male he's interested in quality brand reputation maintaining a professional image and expressing personal style and let me do the same thing for Karen I'm just going to fetch Karen's data and then I'm going to prompt gbt 4 with Karen's data and here we see that gbg4 places Karen in the corporate female category which is based on the name and the bag that she bought now the cool thing about this is that whenever something changes in the data the Persona category is going to be updated and to illustrate this I've made an additional order in Karen's name now she's also purchased the converse top Taylor Shoes of course this signals that Karen might be a mom and it might be a better way to communicate to Karen as a mom instead of a corporate female so if I head back to the notebook and I update Ken data we can see that the data now contains an additional order the toddler Chuck Taylor shoes and if I ask for the Persona category based on this updated dat data I'm going to get an updated guess from the language model so I'll hit the chat completions endpoint again with the updated prompt and here we see that gbt 4 places Karen in the category stylish mom and of course we don't really know what the true Persona is we don't have a Target Persona category we can use for modeling purposes and that's the whole point traditional machine learning might actually have been the wrong tool for this let's have a look at a workflow like this using a more traditional approach like standard machine learning so first you'd have to transform the raw data into feature data that you could feed the machine learning model then you would have to do the modeling in this case it would be some sort of propensity model or customer Affinity model and as I mentioned you don't know the true Persona type so you don't have a Target variable complicating the problem at least making the task non-trivial then you would put rules on on top of the output of the models depending on what type of model You're Building you probably need to map the output or the score of the model to a Persona category then you would need to tag the customer with a suggested messaging and finally marketus would need to formulate the copy for the emails or the advertising now compare this to what we just did we take the raw data pass it to the llm and we get a Persona category and we're not done yet in a second I'm going to take this all the way to the right to serve the business but you already get the point not only is Persona prediction easy with llms but because we ultimately care about taking RI maximizing actions in the form of text the llm is a superior tool for this now let's finish this so we create something that the marketers can actually use if we head over to the customer profile of Karen Backstrom in clavio we can see that we have a section called custom properties and we can enrich Karen's profile with our own scores and predictions so I'm going to tag Karen with a Persona type we can then use those tags to create specific Persona segments that can be targeted in floing campaigns and I can easily do that with the python wber around the Cav API so I'm going to formulate a payload where I add the Persona attributes as properties and of course I'm going to need Ken ID and then I'm going to to use the update profile method so now Karen is tagged with her Persona category so let's head over to clavier again and refresh the customer profile page of Karen Backstrom to see that this is actually the case and here we have the Persona attributes so now we can create segments in clavio based on these Persona attributes like age Behavior gender interest and so on but because we're used using a language model for this we can go even further than that and to do that we'll have a look at how an email marketer formulates a campaign so this is the main web page of Chase Diamond who's running an email marketing agency he's very active on Twitter I'll put a link to his Twitter profile below the video and the reason I'm using Chase Diamond as an example is because he has this beautiful article on using chat gbt for writing the copy for email marketing campaigns and as you can see here he's listing 25 copyrighting Frameworks that you can use used to hook the customer for instance we have the attention interest desire action framework and as you can see here these Frameworks take a customer Persona as input now what I'm going to do is I'm going to take one of these Frameworks and I'm going to inject a Persona type and formulate an email template for a marketing campaign and to do that I'll need a Persona category name I'll prompt the system telling the model that it's a worldclass marketing algorithm that writes personalized emails for fashion brands and then I'm going to ask it to write an email using the attention interest desire action framework and grab the attention of the specific Persona name we're targeting I'm also going to mention a specific product in this case the added Des classic sneakers and I wanted to use the angle increase comfort and then I'll hit the chat completions endpoint with the system prompt and the user prompt and this should give us an email template that we can use for personalization so this is the response from the chat model and here we have a nice email template that can be used to Target the Persona type of Karen which is stylish mom and of course we can just switch out the framework of the product as we see fit so this is nice but we don't want this template in a notebook we want it inside of cavio so I'm going to push it using the API so I'm just going to wrap it in simple HTML and then I'm going to push this using the create template method and of course you can also build a lot more advanced HTML templates so now we have this template inside of clavio and it can be used to Target the messaging to stylish Ms so let's just check that it's in our templates we'll click templates and here we have the top one that was just created so now we have the tax and the customers and we have the templat in the system which is what we need to run targeted communication towards the customers and it's safe to say that if you wanted to automate this using traditional machine learning it would require a lot more work than what we just did so now we have this simple plain template here in glavo we also have some pre-built HTML templates in here so let's continue building on this use case and and move on to the data analysis part so imagine you are a data analyst working for a consumer facing company that uses clayo for email marketing and one day your boss comes into your office and wants to know why some email marketing campaigns are converting really well While others are not performing now this is a big question and it's obviously valuable to figure this out and probably the reason that you were hired as a data analyst in the first place but answering a question like this is like looking for a needle in a hyack there are so many different factors that can affect the performance of an email marketing campaign the HTML templates that are being used in the campaign can affect the conversion rates it could be the font size the font color the text the images the messaging the marketing angle the call to action and so on you really want to AB test everything but where do you start to find the needle in the ha stack you need some sort of magnet but luckily an llm like gpc4 is the magnet you need and to show you what I mean by this let's head back to the collab notebook and extract the data for analysis for this part of the video I'm going to use Lang chain as a wrapper around Omi because Lang chain has a really nice tagging functionality that I'm going to use so here I have a campaign performance data set in adjacent file and you can create a file like this by extracting performance data from the cavio API but I'm not going to burn your time with this as it's it's pretty tedious instead I'm going to use this file and I'm going to make sure that this file is available with the code so I'm loading the campaign performance data into a pandas data frame and here we have the data we have a campaign ID template ID a customer ID the name email the stage of the customer journey and then we have the indicators of whether the email was open clicked and whether or not there was a conversion so with a data set like this you will typically Group by campaign ID and then sum the indicator variables so that you get an idea of the overall performance for each campaign and let me just do that here so we see what that looks like so here we have the aggregated data grouped by campaign ID that allows us to get an overview of what campaigns are performing well and what campaigns are not performing and this is really nice for building dashboards to give you an overview of how your marketing is performing but in order to proove the business we need to go deeper than this we need to understand what it is about the campaign that makes it perform so I'm just going to delete this aggregation again and then let's try to dig deeper using language modeling in order to dig deeper I first need to fetch the templates that we used in the campaigns I'll do that using the clavier API this time I'll just use requests and note that I'm sorting in the URL so I'm getting the last templates created and if I just fetch the first one here we'll see that we get the template that we just created before and I think template number five or number six in the array is one of the pre-built HTML templates and as you can see this one has a lot more going on HTML wise than the one we created before now the reason I wanted to use Lang chain as a WRA around open AI is because Lang chain has a chain type called create tagging chain that is very useful for what we're going to do and Lang chain tagging uses Omar function calling under the hood and allows us to pass a schema and a language model to the chain and then have the chain tag a given document with the features of that document and the document can be in plain text or it can be code like HTML and in order to do the tagging I'll import chat Omi from linkchain chat models and I'll import create tagging chain from Lang chain chains then I'm going to define a schema I'm going to start with the pre-built HTML templates and let's say I wanted to understand how the different features of the template will affect conversion rates such as the font color the background color of the template the call to action that's being used and I can simply specify what features I want to extract from the template in the schema and then the language model will extract those features so if you run the chain on template number five the one we had to look at before see that the chain will extract and return the features in Json format and this is very useful once we add this to the performance aggregation we did before but before I do that let me go ahead and do something similar for the plain text templates we can also extract other types of features such as marketing angles or persuasiveness of the copy and we can extract the underlying copywriting framework the ones we use to generate the email template and note that we can specifically control the values for each property in the schema so I'm going to create a new chain out of this new schema and then I'm going to run this on the first four templates in the template array because these are plain text templates and these are the templates that we used in the campaigns in the performance data set we had in the data frame so I'm just going to create a template dictionary where the keys are the template IDs and the values the features of the template and that allows me to to attach the features of the template to each row in the campaign frame and if we have a look at the final frame here we can see that we now have the features of the template that was used in the specific campaign attached to each row and this allows us to dig deeper into what is really driving the performance of a given campaign so before we could Group by a campaign ID or a template ID but now we can Group by a marketing angle for instance and understand how the different marketing angles affect the performance of The Campaign and here we see that the marketing angle improved status and luxury seems to be an angle that's working really well and of course we can go one step deeper than this we can Group by marketing angle and the customer Journey stage and this breakdown shows us that the marketing angle improved status and luxury Works particularly well for customers in the welcome stage in the welcome flow and this is just one example showing how you combine features extracted using llm function coding with your email marketing performance data to generate actionable insights I hope this video gave you an idea of how om function calling and function calling in general can be used as a powerful data science tool tool if you enjoyed the video like And subscribe thanks for watching\", metadata={'source': 'NBseu9f3P5U'})]\u001b[0m\u001b[32;1m\u001b[1;3mThe Rabbitmetrics YouTube channel covers a range of topics primarily focused on leveraging AI and machine learning for business and marketing purposes. Here are the main topics based on the transcripts of some recent videos:\n",
            "\n",
            "1. **Personalization vs. Segmentation**:\n",
            "   - Understanding the difference between personalization and segmentation.\n",
            "   - Using AI and machine learning to personalize customer experiences.\n",
            "   - The importance of personalization in increasing customer loyalty and revenue.\n",
            "\n",
            "2. **Building BI Dashboards with Large Language Models**:\n",
            "   - Creating and automating BI dashboards using tools like Looker Studio and Google BigQuery.\n",
            "   - Utilizing large language models for generating SQL queries and building views.\n",
            "\n",
            "3. **Langchain Framework**:\n",
            "   - Introduction to Langchain, a framework that combines large language models with external data sources.\n",
            "   - Building applications that are data-aware and can take actions based on user inputs.\n",
            "\n",
            "4. **Knowledge Bases for LLM-based SQL Chains**:\n",
            "   - Building knowledge bases consisting of SQL queries.\n",
            "   - Using Langchain to connect to data sources and generate advanced SQL queries.\n",
            "   - Utilizing Redis as a vector database for storing example queries.\n",
            "\n",
            "5. **Function Calling in LLMs for Customer Analytics**:\n",
            "   - Using function calling to transform traditional data science tools.\n",
            "   - Leveraging OpenAI's GPT-4 for customer analytics and data analysis.\n",
            "   - Automating the extraction of insights from customer data and creating targeted marketing campaigns.\n",
            "\n",
            "These topics indicate the channel's focus on practical applications of AI and machine learning in business intelligence, data analytics, and marketing automation.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What topics does the rabbitmetrics YT channel cover?',\n",
              " 'output': \"The Rabbitmetrics YouTube channel covers a range of topics primarily focused on leveraging AI and machine learning for business and marketing purposes. Here are the main topics based on the transcripts of some recent videos:\\n\\n1. **Personalization vs. Segmentation**:\\n   - Understanding the difference between personalization and segmentation.\\n   - Using AI and machine learning to personalize customer experiences.\\n   - The importance of personalization in increasing customer loyalty and revenue.\\n\\n2. **Building BI Dashboards with Large Language Models**:\\n   - Creating and automating BI dashboards using tools like Looker Studio and Google BigQuery.\\n   - Utilizing large language models for generating SQL queries and building views.\\n\\n3. **Langchain Framework**:\\n   - Introduction to Langchain, a framework that combines large language models with external data sources.\\n   - Building applications that are data-aware and can take actions based on user inputs.\\n\\n4. **Knowledge Bases for LLM-based SQL Chains**:\\n   - Building knowledge bases consisting of SQL queries.\\n   - Using Langchain to connect to data sources and generate advanced SQL queries.\\n   - Utilizing Redis as a vector database for storing example queries.\\n\\n5. **Function Calling in LLMs for Customer Analytics**:\\n   - Using function calling to transform traditional data science tools.\\n   - Leveraging OpenAI's GPT-4 for customer analytics and data analysis.\\n   - Automating the extraction of insights from customer data and creating targeted marketing campaigns.\\n\\nThese topics indicate the channel's focus on practical applications of AI and machine learning in business intelligence, data analytics, and marketing automation.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zHMz8pmmpqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}